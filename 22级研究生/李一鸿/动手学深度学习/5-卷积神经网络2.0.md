## 填充和步幅

### 问题引入：

填充和步幅是卷积层控制输出大小的两个超参数

首先对于填充，假设我们有一个32×32大小的输入图像，卷积核的大小为5×5，由输出形状公式

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220624163237801.png" alt="image-20220624163237801" style="zoom: 50%;" />

可得：

- 第一次输出大小为28×28
- 第七次输出大小为4×4

也就是说当训练到第**7**层神经网络时，我们的输出已经很小很小了，比卷积核还要小。这很明显不符合我们深度学习中“深度”这有概念，我们不想让输入变得那么小

那么如何避免这种情况呢？这里引入**填充**这一解决办法

其次对于步幅，当我们的输入相对大时，假设有224×224的图像输入，卷积核的大小仍为5×5

如果我们想得到一个较小的输出如4×4，根据输出形状公式一层一层不断迭代发现至少需要神经网络的层数为**55**层才能达到目的

我们不希望有那么深的网路，毕竟网络越深意味着计算越多，但我们仍希望得到较小的输出，而卷积核的大小通常为3×3和5×5，所以不希望使用更大的卷积核从而获得较小的输出

那么如何解决这个问题呢？我们可以通过**步幅**来实现，将输出大小与层数的线性关系转换为指数关系

### 填充：

填充是一个很简单的思想，我们在输入的四种加如一些额外的行和列，使得我们的输出比之前更大

下面用公式做一个直观的解释

假设我们的输入仍是32×32大小的输入图像，以及5×5卷积核的大小

（32 - 5 + 1）×（32 - 5 + 1）= 28×28

现在我们填充**p~h~**行与**p~w~**列后我们的输出为：

（32 - 5 + p~h~+1）×（32 - 5 + p~w~1）= (28+p~h~) × (28+p~w~)

==通常取**p~h~** = k~h~ -1 ，**p~w~** = k~w~ - 1==

这里**p~h~** = **p~w~** = 5 - 1 = 4

(28+p~h~) × (28+p~w~) = 32×32

即经过填充后输出大小与输入大小的形状不变，这样的好处是无论如何改变核的大小都不会改变其输出的形状

⚠️其中需要注意的是：

- 当k~h~为奇数时，p~h~为偶数输入图像在上下两侧填充p~h~/2
- 当k~h~为偶数时，p~h~为奇数，输入图像在上下两侧填充一般上测比下侧多填充一行（当然反过来也是可以），所以很少使用偶数的卷积核
- 所填充额外的行和列的元素通常为0

#### 实现：

具体怎么实现填充的，下面代码展示，在所有侧边填充1个像素

~~~py
import torch
from torch import nn


#conv2d是一个2维卷积层,X为需要填充的输入矩阵
def comp_conv2d(conv2d, X):
"""该函数用来方便观测如何放大/减小我们的输出"""
    X = X.reshape((1, 1) + X.shape) #在维度前加一个通道数,和批量大小数保证与卷积层输入参数一致
    #将X重塑为4维
    Y = conv2d(X) #调用2维卷积层得到输出Y,Y为4维输出
    return Y.reshape(Y.shape[2:]) #省略前两个维度:批量大小和通道,得到一个矩阵的输出

~~~

关键字**padding**意味填充，之前填充的p~h~或p~w~是上下或左右一共填充的数字，而对于框架来说padding是一边填充的数字，当padding=1为上下左右各填充1行，当然也可以通过赋值元组来指定上下或左右的填充数

如padding=(2, 1)指上下各填充两行，左右各填充一行

~~~py
conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1) #padding关键字为填充,填充为1
#每边都填充了1行或1列,因此总共添加了2行或2列

X = torch.rand(size=(8, 8)) #随机生成8×8的矩阵

#对比输出大小的形状
comp_conv2d(conv2d, X).shape
torch.Size([8, 8])
~~~

因为我们输入为8×8，核的大小为3×3，(8-3+2+1)×(8-3+2+1) = 8×8，所以输出的大小也为8×8，不会对我们的输入造成变化

当然也可以填充**不同的高度和宽度**

~~~py
conv2d = nn.Conv2d(1, 1, kernel_size=(5, 3), padding=(2, 1)) #padding赋值元组决定行列填充数
comp_conv2d(conv2d, X).shape

#其输出形状:(8-5+4+1)×(8-3+2+1) = 8×8
torch.Size([8, 8])
~~~

### 步幅：

步幅指的是卷积核在输入图像上行/列的滑动步长

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220627211320941.png" alt="image-20220627211320941" style="zoom: 50%;" />

上图为垂直步幅为3，水平步幅为2的二维互相关运算，输入中蓝色🔹标志所移动的位置

很明显输出要小了很多很多，而之前没设置步幅只是向右向下移动一个像素点的位置

通常，当垂直步幅为𝑠~ℎ~、水平步幅为𝑠~𝑤~时，输出形状为

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220627211903203.png" alt="image-20220627211903203" style="zoom:50%;" />

如果我们设置了𝑝~ℎ~=𝑘~ℎ~−1和𝑝~𝑤~=𝑘~𝑤~−1，则输出形状将简化为⌊(𝑛~ℎ~+𝑠~ℎ~−1)/𝑠~ℎ~⌋×⌊(𝑛~𝑤~+𝑠~𝑤~−1)/𝑠~𝑤~⌋

如果输入的高度和宽度可以被垂直和水平步幅整除，则输出形状将为==(𝑛~ℎ~/𝑠~ℎ~)×(𝑛~𝑤~/𝑠~𝑤~)==，因为(𝑠~ℎ~−1)/𝑠~ℎ~ < 1下取整被约掉了

**所以当填充取得比较好时（指输入形状和输出形状相同），输入的高和宽都是2的倍数的话，并且将高度和宽度的步幅设置为2，就相当于将输出的高和宽全部缩小为2了**

#### 实现：

对于步幅，深度学习框架中可以通过stride来设置，它的用法与padding类似，当为一个值时行/列都按该值移动，当为一个元组时，元组第一个值代表行，第二个值代表列

~~~py
conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2) #核仍为3×3
#步幅stride=2即向右移动两列,向下移动两行
comp_conv2d(conv2d, X).shape

#(8/2)×(8/2) = 4×4
torch.Size([4, 4])
~~~

再看一个复杂的例子，对于不对称的矩阵单独对行/列设置填充和步幅

~~~py
conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))

#⌊(8-3+0+3)/3⌋×⌊(8-5+2+4)/4⌋ = ⌊8/3⌋×⌊9/4⌋ = 2×2
comp_conv2d(conv2d, X).shape
torch.Size([2, 2])
~~~

以上就是如何在深度学习框架中使用填充和步幅，加上一个超参数padding,和stride即可

一般来说使用对称的情况，通常来说我们输入的图片是正方形，所以填充是一样的，步幅是一样的

### 小结：

- 填充和步幅是卷积层的超参数
- 填充在输入周围添加额外的行/列，来控制输出形状的减少量
- 步幅是每次滑动核的窗口时的行/列的步长，从而成倍的减少输出的形状

## 多输入多输出通道

前面已经学习了核大小、填充、步幅三种超参数

现在学习另外一个非常重要的超参数——通道

首先来看一下通道的概念，我们之前的学习都是通道数为1的图片，通道数为1的通道可以被称为灰色通道，通道为1的图片是黑白色的，对于简单的图片是ok的

但我们遇到真正的问题大多数都是彩色的图片，而彩色的图片是具有**RGB**三个通道的（red、green、blue），对于彩色图片将任意一个通道单独看就是对应颜色的一张画（如下图所示），三张（三个通道）图片叠加形成了我们看到的彩色图片

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220628171239975.png" alt="image-20220628171239975" style="zoom:50%;" />

而我们将彩色图片转换为灰色的单通道图片就会损失很多信息，所以在输入时我们对图片矩阵需要在引入一个维度——通道数，来实现输入的图像为彩色图像保证图片不会发生信息损失

### 多输入通道：

当输入的图像含多个通道时，需要构造一个与输入数据具有相同输入通道数的卷积核，以便与输入数据进行互相关运算。具体如何进行运算以下图为例：

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220628203353860.png" alt="image-20220628203353860" style="zoom:50%;" />

有两个通道的输入，那么卷积核的输入通道也需要为2。并且将这些卷积核连结在一起可以得到2×2×2的张量，所以当输入有多个通道时，卷积核是一个3d张量

首先通道为0的张量（0,1,2,3）与通道为0的输入进行互相关运算，同理通道为1的张量（1,2,3,4）与通道为1的输入进行互相关运算所得到的结果与通道0的结果相加得到第一个输出，其次通道0或1的卷积核按照单通道计算过程从左向右从上向下按步幅进行滑动互相关运算，最后每次运算两个通道得到的结果相加得到输出

这就是==多输入通道时的计算过程，其结果无论多少通道，输出都为单通道==

#### 实现：

多输入通道的实现其重点就是多通道的卷积核与多通道的输入进行互相关运算

简而言之，我们所做的就是对每个通道执行互相关操作，然后将结果相加。

~~~py
import torch
from d2l import torch as d2

"""
#二维互相关运算
def corr2d(X, K): 
    h, w = K.shape #核的行数与列数
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()       
    return Y 
"""

def corr2d_multi_in(X, K):  
    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))
    #corr2d(X, K)已经存在了d2l中
    #X,K都为3d时,先将X,K zip起来,每次zip时都是对最外侧通道,输入通道数做遍历
    #每次for都会拿出某个通道中的那个小矩阵(x,k)
    #然后对该矩阵进行互相关运算
  	#最后按元素求和
~~~

这就对所有通道输入求和，该函数就是多输入时怎么求互相关

验证一下，以上图为例

~~~py
X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],
               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])
K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])

corr2d_multi_in(X, K)

"""
tensor([[ 56.,  72.],
        [104., 120.]])
"""
~~~

### 多输出通道：

在上节中我们学到，无论输入的通道有多少，最后与卷积核进行互相关运算最后得到输出只有一个通道，然而每一层有多个输出通道是很重要的🦒，为什么重要？

我们在前面提到过，卷积核就像是一个过滤器，它能过滤出图片的某个特征，而一种卷积核只能过滤出一个特征（卷积神经网络1.0—卷积层实现中所举的例子，卷积核只能过滤出垂直边缘而不能过滤出水平边缘），我们在图像识别时，比如实现识别猫猫🐱时，我们自然希望有一种卷积核能对图片进行运算过滤出这只”猫“

但很明显这是不显示的，首先”猫“这个定义就很模糊，并且无法具象化，对于计算机而言模棱两可的东西就是一个灾难。所以我们更倾向于分而治之提取局部特征，比如这是猫耳朵👂，这是猫眼睛👁等，对特征进行细化使得计算机也能够清晰的认识，然后对这些局部特征进行打分，来判断这些特征是不是符合要求。

（下图是李沐老师视频的举例，不同的卷积核过滤出了绿色的瞳色，斜向左的纹理等局部特征....）

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220628212908252.png" alt="image-20220628212908252" style="zoom:40%;" />

而这些不同的特征是需要不同种类的卷积核来实现的，因为你不能训练出一个卷积核过滤出来的特征既是猫👂又是猫👁。所以要有不同种类的卷积核，而对于不同种类的卷积核就会过滤出不同的特征，产生不同的输出，所以单通道的输出并不是我们想要的，因此就需要多通道的输出

直观地说，我们可以将==每个通道看作是对不同特征的响应==

而现实可能更为复杂一些，因为每个通道不是独立学习的，而是为了共同使用而优化的。因此，多输出通道并不仅是学习多个单通道的检测器。现在不管那么多了，如何得到多通道的输出才是关键

#### 实现：

多通道的输出很简单，在多通道输入中我们知道一个3d的张量卷积核与多通道输入互相关运算得到单通道输出，那么我们使用4d的张量卷积核（其中一个维度表示卷积核的种类，其余的维度所代表的含义和多输入通道运算的卷积核一样）这样根据卷积核的种类就能得到多通道的输出，其中==卷积核的种类数 = 输出通道数==，也很好理解，因为卷积核的种类数所提取的种类特征与多个通道的输出相对应

那么计算的原理是什么？

𝑐~𝑖~和𝑐~𝑜~分别表示输入和输出通道的数目，为了获得多个通道的输出，，我们可以为每个输出通道创建一个形状为    𝑐~𝑖~×𝑘~ℎ~×𝑘~𝑤~的卷积核张量，而我们有𝑐~𝑜~种这样的卷积核，所以要共同训练的总的卷积核最后的形状是𝑐~𝑜~×𝑐~𝑖~×𝑘~ℎ~×𝑘~𝑤~

在互相关运算中，每个输出通道先获取所有输入通道，再以对应该输出通道的卷积核计算出结果，下面计算多个通道的输出的互相关函数

~~~py
def corr2d_multi_in_out(X, K):
    #迭代“K”的第0个维度,每次都对不同种类的卷积核k与输入“X”执行互相关运算。
    #在维度0这个方向上将所有结果都叠加在一起,torch.stack()在某个维度方向将张量按顺序堆放一起
    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)
~~~

下面验证一下这个函数是否正确，首先我们需要一个更高维的卷积核

通过将多输入通道定义实现的核张量K与K+1（K中每个元素加1）和K+2连接起来（K中每个元素加2），构造了一个具有3个输出通道的卷积核。

~~~py
K = torch.stack((K, K + 1, K + 2), 0)
K.shape

torch.Size([3, 2, 2, 2])
#3:卷积核的种类数,2:输入通道数,2:行数,2:列数
~~~

然后对输入张量X与卷积核张量K执行互相关运算

~~~py
corr2d_multi_in_out(X, K)

tensor([[[ 56.,  72.],
         [104., 120.]],

        [[ 76., 100.],
         [148., 172.]],

        [[ 96., 128.],
         [192., 224.]]])
~~~

现在的输出包含3个通道，可以发现第一个通道的结果与先前输入张量X和多输入单输出通道的结果一致

### 1×1卷积层：

1×1卷积层指卷积核的高和宽都为1，𝑘~ℎ~ = 𝑘~𝑤~ = 1

它不会识别空间信息，因为每次都只看一个像素，所以它不会识别一个通道中的图片空间模式是什么样子

它的作用只是用来融合不同的通道信息

举个例子：1×1卷积核与3个输入通道和2个输出通道的互相关计算，如下图所示

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220628220524386.png" alt="image-20220628220524386" style="zoom:50%;" />

卷积核每个通道都是1个元素，有两个种类的卷积核（即输出通道数为2），然后进行输出，就得到了两个通道的输出，这样一理顺发现并没有什么意义，如果单看一种卷积核呢？

以浅蓝色1×1卷积核为例，它将3个通道的输入完全融合在了一起而且并没有对每个通道图片的空间做任何处理。

这有什么好处呢？

**我**是这样理解的，以彩色的RGB三通道图片猫猫为例，在最初我们只是对红绿蓝三个通道进行某个特征提取产生了多通道的输出，随着网络的加深，上一层的输出变作了下一层的输入，而刚刚的举得例子猫猫👂还是不够具体，我们还可以将耳朵👂细化出很多局部特征，所以在某一层网络，我们得到耳尖，耳廓等输出。这些输出来作为我们下一层的输入，而下层的输入我们希望得到猫耳朵👂大特征。所以我们需要将这些更微小的特征组合起来从而得到我们猫耳👂，而==1×1的卷积核就将耳尖、耳廓等多通道的输入进行融合==，得到了一个单通道的输出”🐱👂“

可能想多了吧，这个想法还有待求证（有机会问问师哥师姐或者老师🐻‍❄️🐻🦝）

- 当像素为基础应用，1×1卷积层相当于全连接层
- 1×1卷积层通常用于调整网络层的通道数量和控制模型复杂性

#### 全连接实现1×1卷积：

刚刚那个小黑点说明，1×1的卷积等价于全连接

用代码验证一下

~~~py
#定义一个特殊的用全连接实现一个1×1的多输入多输出通道的互相关运算
def corr2d_multi_in_out_1x1(X, K):
    c_i, h, w = X.shape #拿到输入的通道数,每个通道图片的行数和列数
    c_o = K.shape[0] #得到输出的通道数
    X = X.reshape((c_i, h * w)) 
    #对输入进行重新塑形,由于我们以橡素为基础应用,所以不需要考虑空间的结构h*w,将矩阵拉成1维向量
    K = K.reshape((c_o, c_i)) 
    #卷积核形状为c_o×c_i×1×1,利用reshape将后面两个维度拿掉转换为c_o×c_i

    #这样X,K为两个矩阵,直接调用全连接层中的矩阵乘法
    Y = torch.matmul(K, X) #Y的形状为 c_o × h*w
    return Y.reshape((c_o, h, w)) #重新对Y进行塑形还原出最后输出的形状
~~~

这就是用reshape+矩阵乘法转换为全连接层的运算操作来实现1×1卷积核的计算

下面验证一下：

~~~py
X = torch.normal(0, 1, (3, 3, 3))
K = torch.normal(0, 1, (2, 3, 1, 1))

Y1 = corr2d_multi_in_out_1x1(X, K) #利用全连接层的方式进行1×1卷积核的互相关运算
Y2 = corr2d_multi_in_out(X, K) #正常的多输入多输出通道的互相关运算
assert float(torch.abs(Y1 - Y2).sum()) < 1e-6
#最后二者的值相差很小可以说明几乎完全一样
~~~

### 小结：

==**以上代码都是手写实现多输入多输出通道计算，在深度学习框架中直接设置通道的数值即可**==

即以2d卷积层为例，设置输入输出通道参数框架模块中的前向传播即可计算多输入多通道的互相关运算

~~~py
conv2d = nn.Conv2d(3, 2, kernel_size=3, padding=1, stride=2)
#3:为指定的输入通道,2:为指定的输出通道
~~~

这样好简单好简单，所以完全完全不需要手动实现这些多通道多输入的互相关运算啊！

我可真是个大冤种，算啦，多学无坏处

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220628230428138.png" alt="image-20220628230428138" style="zoom:33%;" />

- 输出通道数（卷积核的种类数）是卷积层的超参数
- 每个输入通道有独立的二维卷积核，所有通道相加得到一个输出通道的结果
- 每个输出通道有独立的三维卷积核

## 池化层

我们在（卷积神经网络1.0—卷积层应用）中对图像的垂直边缘检测，当且仅当边缘垂直竖直时我们[1.0, -1.0]的卷积核才能检测出来

但如果在那条垂直的边缘上，哪怕有一个像素点的偏移就会导致这个边缘是检测不出来的，卷积对位置是十分敏感的。很明显我们不希望出现这样的结果，因为在拍照时我们不能保证手是一点都不抖的，或者实际中的照明，物体的位置，比例外观很多很多因素会导致图片中所识别的形象发生变化。

所以我们需要一定程度的平移不变性，物体有稍微的改动输出是不变的，所以我们需要解决这个问题引入**池化层**（Pooling）这一概念，书上说的是**汇聚层**，总之池化层和汇聚层是一个东西

### 最大池化层：

首先看一个**二维最大池化层**这一概念，它的工作原理与卷积层很像，每次有一个窗口然后对输入进行滑动，但与卷积层不同的是，这个窗口并没有一个核函数与输入做点积然后所得元素值求和，而是将窗口中的最大值进行输出

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220629110949684.png" alt="image-20220629110949684" style="zoom:50%;" />

这四个元素为每个汇聚窗口中的最大值：

max(0,1,3,4)=4，max(1,2,4,5)=5，max(3,4,6,7)=7，max(4,5,7,8)=8.

这就是最大池化层工作的原理

==2×2最大池化层可以处理1像素的移位==，所以偏移一点点的话可以通过池化层可以纠正。

### 平均池化层：

- 最大池化层：每个窗口中最强的模式信号
- 平均池化层：将最大池化层中的“最大”操作替换为“平均”

它也是池化层中常用的池化层，直观上来说这两种池化层都是常用操作

### 参数：

池化层的超参数与卷积层的超参数也很类似

- 都具有填充和步幅

这是与卷积类似的两个超参数，但对于其他参数就和卷积层有些出入。

池化层没有可以学习的参数，它的滑动窗口就是一个最大的计算子

在作用多输入通道时，它会对每一个输入通道应用池化层来获得相应的输出通道，与卷积层相比它不会去融合多输入通道，所以池化层最后的==输出通道数 = 输入通道数==。

多通道融合可以交给卷积层来做，池化成的实现就相对简单

这就是池化层与卷积层类似但又不一样的地方

### 实现：

#### 前向传播：

实现池化层的正向传播

~~~py
import torch
from torch import nn
from d2l import torch as d2l

#池化层前向传播函数
def pool2d(X, pool_size, mode='max'): #X为输入,pool_size窗口大小,mode前向传播的模式'max'/'avg'
    p_h, p_w = pool_size #池化窗口的高宽 
    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1)) #创建输出,输出形状与卷积类似
    
    #每行迭代,每列迭代
    for i in range(Y.shape[0]): 
        for j in range(Y.shape[1]):
            #根据模式对滑动窗口中的输入做对应操作
            if mode == 'max': 
                Y[i, j] = X[i: i + p_h, j: j + p_w].max()
            elif mode == 'avg':
                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()
    return Y
~~~

这就是最简单的池化层的前向传播，没有填充和步幅，没有多通道，下面验证一下，以刚刚最大池化层图片为例

~~~py
X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])
pool2d(X, (2, 2)) #模式参数默认为'max'

tensor([[4., 5.],
        [7., 8.]])

pool2d(X, (2, 2), 'avg') #设置模式参数

tensor([[2., 3.],
        [5., 6.]])
~~~

#### 填充和步幅：

利用深度学习框架，介绍一下如何设置池化层的填充和步幅等参数

首先构造一个1×1×4×4的输入张量X

~~~py
X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))
#按顺序1:批量大小,1:输入通道数,4:行,4:列
X

tensor([[[[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.],
          [ 8.,  9., 10., 11.],
          [12., 13., 14., 15.]]]])
~~~

调用pytorch中的nn模组nn.MaxPool2d()，根据输入的数据集创建池化层，如果不去设置步幅==pytorch中的步幅和窗口大小是相同的==

~~~py
pool2d = nn.MaxPool2d(3) #3的意思是3×3的窗口,由于为设置步幅,步幅=窗口大小=3
pool2d(X)

#所以最后的输出为10
tensor([[[[10.]]]])
~~~

如下，窗口中的最大值为10，因为步幅为3所以对于输入X无法滑动窗口

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220629122128440.png" alt="image-20220629122128440" style="zoom: 50%;" />

当然**填充和步幅可以手动设定**

~~~py
pool2d = nn.MaxPool2d(3, padding=1, stride=2)
pool2d(X)

tensor([[[[ 5.,  7.],
          [13., 15.]]]])
~~~

这里就不将输入进行展示了，最后我们可以设定任意窗口的大小，并对行或高分别设定填充和步幅

~~~py
pool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1))
pool2d(X)

#该结果到底是不是真我也不去填充演示了,不过应该是没问题的
tensor([[[[ 5.,  7.],
          [13., 15.]]]])
~~~

所以池化层对这些参数的设置是和卷积差不多的

#### 多个通道：

最后在看一下池化层在每个通道是如何运算的

在通道维度上连结张量X和X + 1(在X的基础上每个元素都加1)，以构建具有2个通道的输入

~~~py
X = torch.cat((X, X + 1), 1) 
#参数意义: (X, X + 1)所要连接的张量, 1:在维度1上进行连接,因为X维度0的含义为批量数
X

tensor([[[[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.],
          [ 8.,  9., 10., 11.],
          [12., 13., 14., 15.]],

         [[ 1.,  2.,  3.,  4.],
          [ 5.,  6.,  7.,  8.],
          [ 9., 10., 11., 12.],
          [13., 14., 15., 16.]]]])
~~~

这样X是一个1×2×4×4的张量，然后对X进行池化层的最大前向传播运算，并将各行各列填充1行，上下左右移动为2列2行，最后池化后输出通道的数量仍然是2。

~~~py
pool2d = nn.MaxPool2d(3, padding=1, stride=2)
pool2d(X)

tensor([[[[ 5.,  7.],
          [13., 15.]],

         [[ 6.,  8.],
          [14., 16.]]]])
~~~

所以池化层的输入通道数 =  输出通道数，它不会做通道融合

### 小结：

- 池化层返回窗口中最大或平均值
- 缓解卷积层位置的敏感性
- 同样有窗口大小、填充、和步幅作为超参数
- 池化层的输出通道数与输入通道数相同

## LeNet（卷积神经网络）

前面学的好多都是为这一节做铺垫！在前面学习了构建一个完整的卷积神经网络的所需组件。

下面介绍**LeNet**，它是最早发布的卷积神经网络之一，因其在计算机视觉任务中的高效性能而受到广泛关注，它在80年代末期银行与邮寄中都有应用，该网络的目的是识别手写数字

所以对于该网络附带一个很出名的数据集——**MNIST**

- 50000个训练数据
- 10000个测试数据
- 每张图像大小28×28像素
- 十类

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220630170141665.png" alt="image-20220630170141665" style="zoom:33%;" />

在当时那个年代MNIST数据集是很大的了，现在该数据集在机器学习中数字识别也经常应用，初学者最早接触的数据集

###  架构：

总体来看，(LeNet（LeNet-5）由两个部分组成卷积编码器和全连接层密集块

- 卷积编码器：由两个卷积层组成;
- 全连接层密集块：由三个全连接层组成。

该架构如下图所示：

![image-20220630170431484](C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220630170431484.png)

下面按照每层的顺序一层一层解释

1. 首先是输入层，输入层对该数据集28×28进行填充，使输入变成32×32
1. 输入到==卷积核大小为5×5的卷积，输出通道为6的卷积层==，经计算得输出为6×(32-5+1)×(32-5+1)=6×28×28
1. 上层6×28×28作为输入，输入到==2×2的池化层==得到6×14×14的输出
1. 上层6×14×14作为输入，输入到==卷积核大小为5×5的卷积，输出通道为16的卷积层==，得到16×10×10的输出
1. 上层16×10×10作为输入，输入到==2×2的池化层==得到16×5×5
1. 以上是**卷积编码器**，上层16×5×5的输出经==展平层==拉成一条向量作为==权重400×120全连接层==的输入
1. 得到1×120的输出，作为==权重120×84全连接层==的输入，得到1×84的输出
1. 上层1×84的输出作为输入，输入到==权重84×10全连接层==得到一个元素为10的向量
1. 该向量经过**softmax回归**将输出规范化得到概率，从而判断出类别，6、7、8为**全连接层的密集快**

### 实现：

深度学习框架实现此类模型非常简单。

只需要知道网络的结构，然后实例化Sequential容器对象，并将需要的层连接在一起就能搭建出letnet模型

模型的结构在上面==高光==都有标出，并且按照顺序进行了标注

⚠️每层输出后都使用了**Sigmoid()**激活函数来保证模型的复杂程度从而避免欠拟合

~~~py
import torch
from torch import nn
from d2l import torch as d2l

net = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),
    #第一层为卷积层,在输入时需要进行填充将28×28输出→32×32输出
    nn.AvgPool2d(kernel_size=2, stride=2),
    #窗口取平均值的池化层
    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),
    nn.Linear(120, 84), nn.Sigmoid(),
    nn.Linear(84, 10))
~~~

将网络从底层到输出可以概括为

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220630204555874.png" alt="image-20220630204555874" style="zoom: 50%;" />

深蓝色为卷积层，浅蓝色为池化层

输入图片张量的维度形状在网络中的变化为： 第一个卷积层使用2个像素的填充，来补偿5×5卷积核导致的特征减少得到28×28的输出。 第二个卷积层没有填充，因此高度和宽度都减少了4个像素。 随着层叠的上升，通道的数量从输入时的1个，增加到第一个卷积层之后的6个，再到第二个卷积层之后的16个。 同时，每个池化层的作用为高度和宽度都减半。最后，每个全连接层减少维数，最终输出一个维数与结果分类数相匹配的输出

### 训练：

**训练使用的GPU，我没有GPU，并且不舍得用电脑CPU跑，所以这一块先跳过了**😿
