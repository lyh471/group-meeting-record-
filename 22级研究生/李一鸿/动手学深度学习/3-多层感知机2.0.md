## 暂退法

暂退法（drop out）又称丢弃法，也是解决过拟合的方法，可能比权重衰减的效果更好

由于好的模型对输入数据具有扰动鲁棒性，在输入的前提上加上一定的噪音数据得到的输出应该是不会影响结果，好的模型是具有一定的抗干扰能力

### 原理：

暂退法就是根据这个性质，在一层到下一层的输出x（通常为隐藏层的输出），对x加噪音得到x‘

- 数据里增加噪音等价于添加了Tikhonov正则
- 丢弃法就是在层与层之间添加噪音

虽然加入了噪音，但我们不喜欢改变期望，即随机下来的x’的平均值仍然是x
$$
E(x') = x
$$
这也是唯一的要求，所以丢弃法根据这个原则对向量x每个元素做以下扰动

规定一个概率p，这个p是超参数

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220620155948028.png" alt="image-20220620155948028" style="zoom: 20%;" />

所以对于在一层得到的输出x，它的每个元素在一定概率上为0，一个概率会被放大，但其期望是不变的

这里可以很简单的证明：
$$
E(x_i) = p*0 + (1-p)\frac{x_i}{1-p} = x_i
$$

### 使用方法：

该方法通常作用在隐藏层全连接层的输出上

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220620160349928.png" alt="image-20220620160349928" style="zoom: 20%;" />

我们得到隐藏层的输出**h**，然后对它进行dropout(**h**)

h中每个元素都有概率放大或变为0

即假设该隐藏层有5个神经元

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220620160539628.png" alt="image-20220620160539628" style="zoom:33%;" />

经过dropout后，h~2~、h~4~变为了0，其他项得到了放大，这就相当于最后的输出将隐藏层h~2~、h~4~单元进行了“**丢弃**”

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220620160754432.png" alt="image-20220620160754432" style="zoom:33%;" />

我们在过拟合那一节中，避免过拟合要降低模型容量，==模型容量的降低可以通过参数的个数和参数取值范围这两个主要的点进行设计，其中权重的衰减是通过控制参数的取值范围来决定的，那么丢弃法就是通过减少参数个数来入手。==从上面的例子可以看出，h~2~、h~4~对应的权重参数被丢弃了，即减少了对应w~2~、w~4~权重的作用，从而减少了参数个数，降低了模型的容量

并且这种丢弃是随机的、概率性的，因为在这一层可能一个单元都不会被丢弃，而在下一层全部被丢弃掉，这样保证了模型的灵活性，对于训练数据在进行输出预测时更多了随机性保证我们的模型不是单纯的记住数据

⚠️但需要注意的是，正则项（对输出加入噪音的项）只在训练中使用，在测试集进行测试预测时不要使用丢弃法，推理过程中丢区分直接返回**h**，即**h** = dropout(**h**)

### 总结：

有很多大佬对这个方法进行了解释，因为每层可能都会去掉几个单元使模型网络相比之前是不完整的，可以看作一个“子网络”，因此在进行训练时，我们拿取了许多子神经网络进行预测，最后得到的是平均值，所以其预测结果是最好的。

但这并没有什么理论依据，从实验数据上的结果类似于正则项，所以归为正则化一类，理论和用法还是有出入的

对于暂退法有以下小结：

- 丢弃法将一些输出项随机值0来控制模型复杂度
- 这个方法常作用在多层感知机的隐藏层输出上
- 丢弃概率是控制模型复杂度的超参数

### 暂退法实现：

虽然从零实现确实能使概念更透彻，但是笔记上就不体现了，学还是要学的，但用还是利用深度框架的高级API来实现

简洁实现很简单，重点看懂模型的构造即可，这里选用两个隐藏层的多层感知机

~~~py
dropout1, dropout2 = 0.2, 0.5

net = nn.Sequential(nn.Flatten(), #展平层,输入展平
        nn.Linear(784, 256), #输入784,输出256
        nn.ReLU(), #对输出256进行激活函数,第一个隐藏层的输出作为下一层的输入
        nn.Dropout(dropout1), #在第一个全连接层之后添加一个dropout层,对隐藏层输出进行概率丢弃
        nn.Linear(256, 256), #上一层丢弃后得到的数据作为输入,进行输出
        nn.ReLU(), #对第二层的输出进行激活
        nn.Dropout(dropout2), #在第二个全连接层之后添加一个dropout层,为下一层的输入做概率丢弃
        nn.Linear(256, 10)) #输出层

def init_weights(m): #权重初始化函数
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights); #递归调用权重初始化函数
~~~

训练结果啥的就不分析测试啦🐻‍❄️

## 数值稳定性：

随着神经网络模型越深，层数越来越多，数值会产生不稳定性。

假设有n层神经网络，在进行训练时是需要对梯度求导的，而每层的权重是向量，损失函数向量对向量的求导是矩阵，导数是由链式求导法则得到的。

所以当神经网络的层次越深，n的数值越大，在根据链式求导法则时需要对矩阵进行累乘，如果梯度数值过大就会发生梯度爆炸现象，如果梯度过小就会发生梯度消失。

比如极端的情况下，链式求导时中间梯度全为2或0.1，神经网络的层数为100，最后梯度结果为2^100^或(0.1)^100^

这两个数都撒很恐怖的数，要么太大，要么太小💀使数值不稳定，同时这两种极端的情况还会带来很大的隐患

### 梯度爆炸：

- 值超出值域，太大的浮点数我们根本无法表示
- 对学习率十分敏感
  - 如果学习率太大→大的参数值→更大的梯度
  - 如果学习率太小→训练无法进展
  - 在训练过程中需要不断调整学习率

### 梯度消失：

- 梯度值变为0，对16位浮点数尤为严重
- 训练没有进展
  - 不管如何选择学习率
- 对底层尤为严重
  - 仅仅顶层训练的较好，因为求导是根据反向传播，首先对输出层求导
  - 无法让神经网络更深

### 合理的模型初始化：

既然发现了问题，我们就要努力去规避，因此接下来从模型初始化过程中来避免数值不稳定这个问题

我们希望数值稳定，希望每层的输出和梯度在一个合适的范围。

其中一个想法即让每层的输出和梯度都看作随机变量，然后让它们的均值和方差都保持一致，这样不管是多深得网络，第一层和第n层的数值都是差不多的

我们希望输出和梯度这两种随机变量均值为0，方差为一个常数，这样就把这个变量的数值圈定在一个范围从而保证了数值的稳定性，这是一种假设，我们希望设计的神经网络满足这个性质

所以，可以利用合适的权重初始化来满足这个性质，为什么合理的权重初始化能避免数值不稳定？

因为刚开始训练时，我们选择的权重可能距离最优值还很远，把两点看作山底和山坡的两点，它离谷底还有一段距离，并且该点到山底的方向还非常陡峭，陡峭就意味着梯度较大，而梯度越大越容易发生梯度爆炸。相反，如果是在最优解附近初始化权重，越距离山底表面越平缓。所以合理的权重初始化可以避免数值不稳定性。

前面的权重初始化，我们一般选择的为均值为0，方差为1的正态分布N(0 , 0.01)，对于小网络是没有问题的，但不能保证深度神经网络，这种初始化随机可能会导致梯度爆炸或梯度消失

*我很想讨论一下，这个过程正向方差如何推导出来的结论，如果我有pad的话（卖掉了）我会选择写写，数学公式用电脑真是太难敲了！！！！*😿*填个坑，等我买个新的pad一定把这一块给补上*

所以直接上**结论**

#### 对于无激活函数的网络：

只有当n~t-1~\*r~t~ = n~t~*r~t~ 时，就能保证每一层方差都是一个常数（其中n~t-1~表示第t层输入向量的维度，n~t~表示第t层输出向量的维度，r~t~为第t层的方差）==现在暂时记住就行==

但该等式很难实现，因为第t层的输入维度和输出维度一般都是由网络决定的，并且不一定相等，所以我们可以做一步权衡，做一步退让——**Xavier初始化**，我们使我们权重初始化的方差为2/(n~t-1~+n~t~)
$$
r_t(n_{t-1}+n_t)/2 = 1 \\\\
r_t = 2/(n_{t-1}+n_t)
$$
这样保证权重的初始化时，可以有效的使数值稳定

那么权重初始化时可以遵循

- 正态分布 ***N( 0 , √2/(n~t-1~+n~t~) )***
- 均匀分布***u( -√6/(n~t-1~+n~t~) , √6/(n~t-1~+n~t~) )***
  - 均匀分布u(-a , a)，方差是a^2^/3

权重初始化适配形状变换，权重由维数决定

#### 对于有激活函数的网络：

也是直接上结论，对于有激活函数的网络，如果要满足均值为0且每一层方差都是一个常数，这激活函数必须过原点。并且激活函数等于它本身，即σ(x) = x

我们研究一下ReLU、Sigmoid和tanh的泰勒展开

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220621162550455.png" alt="image-20220621162550455" style="zoom: 50%;" />

除了sigmoid函数，另外两个函数在0点附近基本都满足σ(x) = x，而幸运的是，我们的==权重所取得值也是在0点附近==，所以这两种激活函数更容易使数值稳定

但我们可以调整一下sigmoid函数的结构，使该函数的泰勒展开近似为x

调整sigmoid：

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220621162943970.png" alt="image-20220621162943970" style="zoom: 50%;" />

调整后的sigmoid函数也很好用，所以选择合理的激活函数也是保证数值稳定的关键

### 小结：

- 当数值过大过小时都会导数数值问题
- 这种现象常发生在深度模型中，因为其会对n个数据累乘

⚠️ReLU函数更容易发生梯度爆炸，Sigmod函数容易发生梯度消失

下面是几个核心思想来解决数值的不稳定性

- 乘法变加法
  - 比如100次乘法转换成100次加法，在ResNet，LSTM中均有应用
- 梯度归一化
  - 梯度归一化
    - 比如将梯度变为均值为0方差为1的数，无论梯度多大，都将梯度拉到这个范围内
  - 梯度裁剪
    - 比如梯度大于5时，将梯度取5；梯度小于-5时梯度取-5
- 合理的权重初始化和激活函数

权重的初始化和激活函数在上面合理模型的初始化中讲过了，上面两种思想以后再说
