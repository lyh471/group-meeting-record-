## softmax回归实现

### 损失函数：

CrossEntropy（交叉熵）

在回归原理中，交叉熵可以衡量训练出的模型的拟合程度，神经网络模块中有优化好的API交叉熵损失函数可以直接调用

#### torch.nn.CrossEntropyLoss()

CrossEntropyLoss()损失函数，是将Log_Softmax()激活函数与NLLLoss()损失函数的功能综合在一起了
$$
c r o s s \_ e n t r o p y = \log\_ s o f t m a x + n l l l o s s
$$

ok打住，底层逻辑啥的现在不用太过深入，这个接口会用就行！

- **input** -- 预测值
- **target** -- 目标值
- **reduction** -- 对输出进行指定，'none' |  'mean' | 'sum'，三个可选项，none对输出不做改变，mean对输出求均，sum对输出求和

 ~~~py
 loss = nn.CrossEntropyLoss(reduction='none') #输出不做改变
 l = loss(y_hat, y) #预测值和标签做交叉熵
 ~~~

### 优化算法：

优化迭代算法使用学习率为0.1的小批量随机梯度下降作为优化算法

这与我们在线性回归简洁实现例子一致，说明了优化器的普适性

~~~py
trainer = torch.optim.SGD(net.parameters(), lr=0.1)
#输入net模型中的各参数,将学习率设为0.1
~~~

调用优化函数后，trainer就是一个optimizer（优化器），可以调用优化器中的方法

### 训练：

定义训练函数，训练后的结果书上定义可视化预览，我还不会用**matplotlib**，就不在深究

首先定义训练模型一个迭代周期，该函数一次迭代传输的算法优化函数参数是一个函数，参数名为**updater**

⚠️**updater**是更新模型参数的常用函数，它接受批量大小作为参数，既可以是包中d2l.sgd的函数，也可以是框架的内置优化函数

~~~py
def train_epoch_ch3(net, train_iter, loss, updater):  #训练模型,训练集,损失函数,批量大小
    """训练模型一个迭代周期（定义见第3章）"""
   	
    if isinstance(net, torch.nn.Module):  
    #isinstance()是Python中的一个内置函数，用来判断一个对象的变量类型
        net.train() 
    #该语句判断模型训练是自定义还是使用的pytorch的nn模组,若为nn模组,将模型设置为训练模式,可求梯度  
      
    metric = Accumulator(3) #定义的累加器类,实例化出3个对象,在d2l中自定义的类,之后很有可能复用
    #分别为: metric[0]训练损失总和、metric[1]训练准确度总和、metric[2]样本数
    
    for X, y in train_iter:
        # 计算梯度并更新参数
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):
            # 对updater进行判断,若使用PyTorch内置的优化器和损失函数,则进行成套老三样
            updater.zero_grad() #每次批量更新迭代时,清空上次计算梯度
            l.mean().backward() #计算该批量下损失函数的梯度
            updater.step()		#梯度下降,对参数更新
        else:
            # 不是使用的nn模块进行简洁化实现,而是从零开始的自定义实现,调用自定义优化算法函数
            l.sum().backward()
            updater(X.shape[0])
                
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
        #float(l.sum())小批量损失放入metric[0]累加器中进行累加
        #accuracy(y_hat, y)小批量精度放入metric[1]累加器中进行累加,自定义的计算精度函数
        #y.numel()小批量样本数放入metric[2]累加器中进行累加
   	#循环结束,得到损失、训练准确度、样本数总和
        
    #返回训练损失和训练精度
    return metric[0] / metric[2], metric[1] / metric[2]
~~~

在展示训练函数的实现之前，书上定义了一个在动画中绘制数据的实用程序类**Animator**， 它能够简化书中其余部分的代码

这就是我说的利用**matplotlib**，将训练结果可视化的动画类

接下来我们实现一个训练函数， 它会在train_iter访问到的训练数据集上训练一个模型net。 

该训练函数将会运行多个迭代周期（由变量num_epochs指定）

 在每个迭代周期结束时，利用测试数据集test_iter访问到的测试数据集对模型进行评估

 利用**Animator**类来可视化训练进度。

~~~py
def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  
    """训练模型（定义见第3章）"""
    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
                        legend=['train loss', 'train acc', 'test acc'])
    #可视化
    
    #epoch迭代次数,遍历数据集的次数
    for epoch in range(num_epochs):
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
        #每循环一次,计算出训练损失和训练精度赋值给train_metric
        test_acc = evaluate_accuracy(net, test_iter)
        #测试数据集评估出的精度
        animator.add(epoch + 1, train_metrics + (test_acc,))
        #训练可视化
   
    train_loss, train_acc = train_metrics
    #一次迭代后,train_metric(训练损失,训练精度)张量的两部分,分别赋值给train_loss,train_acc
    
    """异常处理"""
    assert train_loss < 0.5, train_loss
    assert train_acc <= 1 and train_acc > 0.7, train_acc
    assert test_acc <= 1 and test_acc > 0.7, test_acc
~~~

调用d2l中写好的训练函数，迭代10次，训练后的精度比之前高

~~~py
num_epochs = 10
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
~~~

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220614211348352.png" alt="image-20220614211348352" style="zoom: 50%;" />

# 多层感知机

## 感知机

人工智能最早的一个模型，它的结构很简单

- 给定输入**x**，权重**w**，和偏置*b*，感知机输出有

$$
o = \sigma(<w,x>+b) \quad \quad \sigma(x) =c(u)=\begin{cases} 1,\quad if \quad x > 0\\ -1 \quad otherwise \end{cases}
$$

### 训练：

它解决的是一个二分类问题

给数据集打上标签，按照上述感知机模型，训练集标签分为**1**类和**-1**类，根据输入特征计算后通过激活函数得到输出，若[<w,x>+b] <= 0，且真实标签值y = 1，则说明分类失败；同理若预测值*o* > 0，标签值[<w,x>+b] > 0，且真实标签值y = -1，同样分类失败。分类失败说明预测值与真实标签值有损失，需要利用梯度下降重新训练出新的权重和偏置直到分类成功

具体有：

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220613210653324.png" alt="image-20220613210653324" style="zoom: 25%;" />

- 初始化权重和偏置为0
- 遍历整个数据集从中选取1个进行训练，若真实标签值 y 与 [<w,x>+b] 的乘积 <= 0 说明分类失败，需要更新权重和偏置（分类成功二者乘积大于0，不信可以自己试试）
- 直至权重和偏置使分类成功，从训练集中选取下一个数据重复上条步骤（权重和偏置是继承上条步骤训练的结果的）

该过程等价于使用批量大小为1的梯度下降，只有分类失败时才有损失

**损失函数：**
$$
l(y,\bold x,\bold w) = max(0,-y<\bold w,\bold x>)
$$
分类正确损失函数为0，分类不正确损失函数不为0需要迭代权重和偏置

### 收敛：

收敛定理指出感知机训练何时停止，具体定理公式不在解释啦，暂时用不到，用到再说。

按理说就是一直一遍又一遍的遍历训练集，从几何上说找到一个线性切割面可以将两个类别进行分割，切割的两部分是分出的两类，这条线性切割面就是我们要训练出的模型

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220613212924938.png" alt="image-20220613212924938" style="zoom: 50%;" />

但现实往往不可能一帆风顺，感知机对**XOR**问题束手无策

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220613213325392.png" alt="image-20220613213325392" style="zoom:25%;" />

对🟢球和🔴球进行分割，而感知机只能产生线性的切割面（只能直着切一刀）是不能进行分割的，除非是一条曲线（非线性）

结果导致了AI的第一个寒冬，做了一个那么复杂的机器模型连个简单的XOR函数也不能拟合，大家就对其他方向研究.......直至**多层感知机**的出现！

### 小结：

- 感知机是一个二分类模型，最早的AI模型之一
- 求解算法等价于使用批量大小为1的梯度下降遍历整个训练集
- 不能拟合XOR函数，导致第一次AI寒冬

## 多层感知机

上节提到感知机并不能拟合XOR函数，因为感知机只能产生线性的切割面

而如何解决这个问题？多层感知机！通过叠加感知机并结合激活函数将模型产生几何意义上的线性切割曲面转换为非线性的切割面从而达到分类的效果

### 隐藏层：

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220614212631600.png" alt="image-20220614212631600" style="zoom:33%;" />

上图可知，分为3个层次，输入层、隐藏层和输出层

首先输入层和输出层的形状是由数据决定的，根据数据的输入特征输入层需要几个单元，根据最后输出的结果得到输出单元的个数

我们输入数据得到输出，直觉上就两个层次，输入层和输出层

但是这是删除复杂的细节抽象出来的结果，因为在处理复杂的分类问题和庞杂的信息时，单单的线性传递是不够的

我们需要对输入的数据特征进行维度上的改变，从而能够进行更细致的划分

举个简单的例子，还是猫猫🐱识别，我们有10个特征信息，而最后的输出只有一个是猫或者不是猫。如果直接由输入得到输出，即10个特征经过线性变换模型得出最后的结果，这个结果往往是很粗糙的。我们更希望将这10个特征更加细致化的使用，而这就是隐藏层的作用，在输入层和输出层之间再加上一层，把输入数据的特征，抽象到另一个维度空间（10 → 6），来展现其更抽象化的特征，使抽象后的特征能更好的进行线性划分

这就是隐藏层的作用，但是从直觉上说由输入直觉得到了输出，该层次被隐藏起来了

举一个单隐藏层—单分类例子

- 输入 **x** 为n维特征向量
- 隐藏层权重矩阵**W~1~** mxn ,，偏置*b~1~* 长度为m的向量
- 输出层 **w~2~** mx1 ，偏置*b~2~* 为标量

$$
\bold h = \sigma(\bold W_1\bold x + b_1)\\
o = \bold w_2^T\bold h + b_2
$$

其中 **h** 作为隐藏层的输出作为输出层的输入，该过程通过隐藏层将n维特征抽象到了m维

**σ(x)**是一个激活函数，下面详细的讨论一下它🐻‍❄️

### 激活函数：

σ(x)它会对输入的每个元素进行激活（按元素激活），该函数一定是非线性的

- 什么是激活函数？

我们引入多层感知机就是使我们建立的模型从几何上来说能产生非线性的切割面对数据进行分类，隐藏层可以理解为对数据输入特征维度的抽象化处理，但从根本上并没有解决本质上的问题，模型本身仍然只能进行线性切割，所以需要一个函数将数据非线性化，并将庞杂的信息压缩到某个区间上，这个函数就是激活函数

- 为什么激活函数必须是非线性的？

如果是线性的激活函数，就等价于应该单层的感知机，这对于解决的问题毫无意义甚至可以说是模型退化。所以激活函数一定是非线性的

线性变换后对每个隐藏单元应用非线性的*激活函数*𝜎。

 激活函数的输出被称为*活性值*， 一般来说，有了激活函数，就不可能再将我们的多层感知机退化成线性模型

下面举几个常用的激活函数

#### ReLU函数：

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220614220213477.png" alt="image-20220614220213477" style="zoom: 50%;" />

给定元素𝑥，ReLU函数被定义为该元素与0的最大值

这是一个最简单的非线性函数，σ(x) = x是线性的不行，但这个函数很简单，如何转换为非线性呢？砍一刀🔪，把小于0的部分砍掉

这个函数很简单很简陋，但好处是计算的很快，指数函数运算是很耗cpu的，所以在构建模型时想不到好的激活函数，选它就对了

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220614220602035.png" alt="image-20220614220602035" style="zoom: 45%;" />

当然该函数有很多变体，比如在此基础上加上一个偏置项，实际应用的时候可根据情况自行调整

#### sigmoid函数：

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220614220828386.png" alt="image-20220614220828386" style="zoom:50%;" />

sigmoid通常称为*挤压函数*， 它可以将范围（-inf, inf）中的任意输入压缩到区间（0, 1）中的某个值

在做二分类问题时，可以作为输出单元的激活函数将输出转换为概率， ==可以将sigmoid视为softmax的特例==。 然而，sigmoid在隐藏层中很少使用， 它在大部分时候被更简单、更容易训练的ReLU所替代

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220614221229899.png" alt="image-20220614221229899" style="zoom:45%;" />

#### tanh函数：

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220614221337582.png" alt="image-20220614221337582" style="zoom:50%;" />

与sigmoid函数类似，tanh(双曲正切)函数也能将其输入压缩转换到区间(-1, 1)上。

当输入在0附近时，tanh函数接近线性变换，函数的形状类似于sigmoid函数，不同的是tanh函数关于坐标系原点中心对称

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220614221453255.png" alt="image-20220614221453255" style="zoom:45%;" />

先简单介绍这3个激活函数，后续若再有就继续进行补充

## 多层感知机实现

### 从零开始实现：

多层感知机**MLP**的原理已经熟知，按理来说接下来直觉使用框架简洁实现，但从零开始的实现也并不复杂，所以在调用API简洁实现前，可以先从零开始实现一下

仍然以softmax回归为例

~~~py
import torch
from torch import nn
from d2l import torch as d2l 

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)#获得训练集和测试集
~~~

#### 初始化模型参数：

回忆图像的大小是28*28 = 784灰度像素值组成，所有图像共分为10类，忽略空间结构可以将图像视为784个输入特征和10个类别输出

实现一个单隐藏层的多层感知机，该隐藏层包含256个隐藏单元（隐藏层的层数和每层隐藏单元的数目都视为超参数），即将784个特征抽象细化成256个特征

==一般选择2的若干次幂作为层的宽度，因为内存在硬件中的分配和寻址方式，这么做往往可以在计算上更高效==

~~~py
num_inputs, num_outputs, num_hiddens = 784, 10, 256 # 每个层的宽度

W1 = nn.Parameter(torch.randn(
    num_inputs, num_hiddens, requires_grad=True) * 0.01)
#*0.01保证随机产生的数据符合均值为0方差为0.01的正态分布
#nn.Parameter()可有可无,只是用来说明为神经网络的参数

b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))
W2 = nn.Parameter(torch.randn(
    num_hiddens, num_outputs, requires_grad=True) * 0.01)
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))

params = [W1, b1, W2, b2]
~~~

以上参数对应分别为

- 隐藏层权重矩阵 **W~1~** ，偏置*b~1~*
- 输出层权重矩阵 **w~2~** ，偏置*b~2~* 

#### 激活函数：

模型参数初始化之后，选取激活函数，将线性→非线性

这里使用ReLU作为激活函数，并且自定义该函数

~~~py
def relu(X):
    a = torch.zeros_like(X) #a为形状与x相同的元素全为0的张量
    return torch.max(X, a)
~~~

#### 模型：

由于忽略了空间结构，在输入的时候将28*28的灰度像素矩阵拉成一个长784的向量进行输入，根据单层多层感知机的公式，几行就能构建模型

~~~py
def net(X):
    X = X.reshape((-1, num_inputs)) # num_inputs = 784,shape = (1,784)
    H = relu(X@W1 + b1)  # 这里“@”代表矩阵乘法,点乘,转不转置看你怎么定义张量的形状
    return (H@W2 + b2)
~~~

#### 损失函数：

softmax回归简洁实现直接调用的高级API接口的损失函数，这里主要是感知机的从零开始的实现，完全没有必要自定义损失函数，直接调用torch.nn.CrossEntropyLoss()即可

~~~py
loss = nn.CrossEntropyLoss(reduction='none')
~~~

#### 训练：

训练过程和训练函数与softmax回归，只是我们对模型net做了优化，直接将重构的net作为参数训练即可

~~~py
num_epochs, lr = 10, 0.1
updater = torch.optim.SGD(params, lr=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)
~~~

其结果相比为加多层感知机的模型来说，损失相比更小了，精度更高了

如下图所示（可与 softmax回归实现—训练做对比）

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220615164356132.png" alt="image-20220615164356132" style="zoom:50%;" />

### 简洁实现：

 通过高级API简洁地实现MPL

~~~py
import torch
from torch import nn
from d2l import torch as d2l
~~~

模型定义：

~~~py
net = nn.Sequential(nn.Flatten(), #输入展平,忽略输入的空间结构将输入拉成一个向量
                    nn.Linear(784, 256), 
                    #一个线性全连接层,从图形上可以理解为两个层
                    #784为输入层,256作为隐藏层
                    nn.ReLU(),	#对隐藏层,使用ReLU作为激活函数
                    nn.Linear(256, 10)) 
					#也可以理解为两个层
    				#256隐藏层的输出作为10输出层的输入

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights); #函数fn递归地应用到模块自身以及该模块的每一个子模块中,对参数进行初始化
~~~

训练前定义学习率和训练批量大小、迭代次数、训练集、测试集以及优化算法

~~~py
batch_size, lr, num_epochs = 256, 0.1, 10
loss = nn.CrossEntropyLoss(reduction='none')
trainer = torch.optim.SGD(net.parameters(), lr=lr)
~~~

传参训练，寥寥几行就完全实现了多层感知机的定义，十分滴方便

~~~py
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
~~~

##  模型选择

无论是预测模型还是分类模型，我们都希望得到的是一个泛化模式，而不是一个只记住数据的傻子

什么叫只记住数据的傻子？

比如有一个图片的特征为***T***，它的分类是猫猫🐱

我们不希望我们的模型看到数据特征***T***后，就喊道：“看，这个数据是猫！”

我们更希望我们的模型不是单纯的拟合数据记住数据，而是在碰到从未见过的特征时有自己的判断，而不是训练的时候记住整个数据集。只有当模型真正发现一种泛化模式时，才会作出有效预测

### 训练误差和泛化误差：

为了进一步讨论这一现象，引入两个新的概念，训练误差和泛化误差

两个备战高考的学生努力的准备考试，一个学生做了大量的题目，完全记住了题目答案，而另外一个学生将问题学透学明白了，在模拟考试时（大量原题），两人分数一样，但当高考时（全新的考试，没有原题），靠死记硬背答案的学生与另外一名试图理解题目并尝试解决答案的学生相比，往往后者会考得更好

所以从这个例子中，模拟考试成绩可以理解为训练误差，而高考成绩为泛化误差

**训练误差**： 模型在训练数据集上计算得到的误差

**泛化误差**： 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。（测试误差权重平均值）

但问题是永远不能准确地计算出泛化误差，因为无限多的数据样本是一个虚构的对象

所以在实际应用时只能通过将模型应用于一个独立的测试集来估计泛化误差， 该测试集由随机选取的、未曾在训练集中出现的数据样本构成。

### 验证集和测试集：

我们在训练数据后，往往会对模型预测的精确度进行评估，从而选择较好的模型

我们当然希望选择泛化误差更小的模型，泛化误差在实际应用中可以通过独立的测试集来估计泛化误差

但在训练多层感知机模型时，我们可能希望比较具有不同数量的隐藏层、不同数量的隐藏单元以及不同的的激活函数组合的模型。

但原则上，我们在确定所有的超参数之前，我们不希望用到测试集，==测试集理解为高考，只有一次机会，考完之后在使用该测试集得到的误差就不具有泛化性了，因为会有过拟合的风险，模型将测试集数据记住（高考题背下来了！）==，但既然学习了新的知识（训练出新的模型）需要对知识进行检验评估该怎么办呢？这就要引入验证集这一概念，==验证集可以理解为模拟考试==

**验证集：**

用于检验训练过程中检验模型的状态，收敛情况，验证集通常用于调整超参数，根据几组模型验证集上的表现决定哪组超参数拥有最好的性能

同时验证集在训练过程中还可以监控模型是否发生过拟合，即一般来说验证集表现稳定后若继续训练，训练集表现还会继续上升，但是验证集出现不升反降的情况，这样一般就发生了过拟合，验证集可从此判断何时停止训练

**测试集：**

用于训练结束后对模型进行测试，评估泛化能力。

我们先通过训练集训练得到了模型，然后根据验证集确定了超参数，最后使用一个从未见过的数据集来判断这个模型的好坏，而这个数据集就是测试集

⚠️==无论验证集和测试集都不参与训练，只是起到评估作用==

⚠️训练集、验证集、测试集三者的数据是不一样的！是三个不同的数据集，一定要注意这一点

但当数据集较小时，可以利用**K折交叉验证**来将数据集划分为训练集和验证集

#### 欠拟合和过拟合：

当我们比较训练和误差时，还需要注意两种常见的情况：

（1）训练误差和验证误差都很严重， 但它们的差距很小。 如果模型不能降低训练误差，可能意味着模型过于简单（即表达能力不足）， 无法捕获试图学习的模式。 并且由于训练和验证误差之间的泛化误差很小， 我们有理由相信可以用一个更复杂的模型降低训练误差。 这种现象被称为**欠拟合**

（2）当我们的训练误差明显低于验证误差时， 这表明严重的**过拟合**。训练模型完全把训练数据集给记住了，而对于未知的数据不能做很好的预测，就像死记硬背的考生在面对新题型的考试不能做到灵活的变通。但需要注意的是，**过拟合**并不总是一件坏事。 特别是在深度学习领域，众所周知， 最好的预测模型在训练数据上的表现往往比在保留（验证）数据上好得多。 最终，我们通常更关心验证误差，而不是训练误差和验证误差之间的差距。

以上就是欠拟合和过拟合的概念，可以用一个图简单的总结一下

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220615221022352.png" alt="image-20220615221022352" style="zoom: 25%;" />

当模型容量过低，数据复杂度过高会出现欠拟合现象，可以考虑换个复杂度更高的模型

当模型容量过高，而数据复杂度较低，会出现过拟合现象，适当的降低模型的复杂度

**数据的复杂度：**

数据的复杂度，数据的复杂度是很难定义的，但有多个重要因素

- 样本数
- 每个样本的元素个数
- 时间、空间结构（股票预测具有时间结构、图片具有空间结构、视频具有时空结果）
- 多样性（数据的类别，十类、百类、一千类）

**模型容量：**

- 模型的容量指的是拟合各种函数的能力（非线性拟合函数的能力远大于线性函数的拟合能力）
- 过低容量的模型难以拟合训练数据
- 过高容量的模型可以记住所有训练数据

所以模型的过高和过低都不太好，影响模型容量一般来说有两个因素

- 参数个数。当可调整参数的数量（有时称为*自由度*）很大时，模型往往更容易过拟合。
- 参数值的选择范围。当权重的取值范围较大时，模型可能更容易过拟合

### 总结：

铺垫了那么长时间，我们应该选择什么样的模型呢？一张图就可以概括

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220615223403762.png" alt="image-20220615223403762" style="zoom: 50%;" />

即最佳的模型是如上图的虚线，它需要抓住两点核心问题：

- 尽可能的降低泛化误差
- 泛化误差和训练误差的差量尽量小

## 权重衰减

==神经网络的本质就是利用简单的感知机嵌套（多层感知机），然后可以拟合出我们所需要的任何一种函数，或者说任何一种曲线==，我们不断地在训练中过度的嵌套感知机，不节制的训练参数，往往会得到接近零误差的函数，而这就是过拟合。过拟合的模型泛化能力往往很弱，所以我们希望避免产生过拟合来提高模型的泛化能力，那么就需要人为的干预调节模型的容量，及时止损，防止训练过度是模型容量过高从而产生过拟合，而这个过程叫做**正则化**

### 正则化：

⚠️首先正则化和正则表达式并不是一回事，二者名字差不多也只是一种巧合😿千万不要混淆

**定义：**凡是减少泛化误差，而不会减少训练误差的方法，都可以称作为正则化的方法

概括的说就是凡是可以减少过拟合的方法都叫做正则化方法

#### **参数正则化：**

回到刚刚的问题，为了避免训练过的而产生过拟合。我们需要适当的调整模型容量，而影响模型容量主要有两个因素（1）参数取值（2）参数数量，本节权重衰减就是通过限制参数值的选择范围来控制模型数量，该方法可以定义为参数正则化

在神经网络中主要的参数为==权重==和==偏置==，所以参数正则化就是对权重和偏置的取值进行限制

而**W**权重是模型未知数的系数，权重的取值决定曲线的样子，*b*偏置取值并不会改变曲线的样子，只会影响曲线的平移情况。所以参数正则化一般是对权重**W**而言，对偏置意义不大

下面将介绍对权重正则化的一个方法——**权重衰减**

### 权重衰减：

在训练参数化机器学习模型时， *权重衰减*（weight decay）是最广泛使用的正则化的技术之一， 它通常也被称为𝐿~2~正则化。

下面简单概括一下什么是权重衰减

首先我们不节制的训练，大量的迭代肯定会对训练集数据产生一个拟合饱和的曲线（过拟合），只看这条曲线，这个曲线一定是可以由函数的方式表达出来的（神经网络也可以理解为一个经过大量函数嵌套的函数模型）

而只要可以通过函数的方式表达出，那么就可以进行泰勒展开

假设有一条拟合出的曲线函数f(x)，然后对该曲线进行泰勒展开有一个这样的表达式

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220616215728013.png" alt="image-20220616215728013" style="zoom: 25%;" />

这是一个多项式函数，函数有x的一次项、二次项、三次项......n次项

当函数只有一次项时，f(x)泰勒展开它就是一个直线，当x有二次型时，f(x)泰勒展开就代表了一个弯是一条曲线，x的三次项就具有两个弯....甚至有x具有n次项那么就有n个弯

曲线拐的弯越多，越说明模型更复杂，更说明模型容量更高。所以要减少模型的容量，曲线的弯弯绕绕，就是尽可能的减少高此项对整个函数的贡献

如何减少函数高次项对函数贡献呢？即对函数高次项中的系数下狠手！👊如果前面系数为0，那么这一项就没有贡献、高次项的系数数值越小对函数贡献越小

所以通过调节系数达到正则化的目的，即使高次项前面的系数不要太大，而高次项的系数是什么？

f(x)的导数，（从上式可以看出），我们选择任意一个常数代入（比如上式常数a~0~），那么f(a~0~)就可以看作关于权重**W**的函数（在f(x)中，当x确定后权重是未知的）

*绝妙的布道！赞美王木头👑🪵*

有一个关于极限的定义，就是当权重**W**趋近于0，那么f(x)的n阶导数也趋近于0

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220616223306766.png" alt="image-20220616223306766" style="zoom:25%;" />

这里王老师没有证明，我也就不证明了，总之从这个极限可以看出**W**取值越小，高次项系数的值也就越小

所以控制**W**越小，就能使模型容量降低

==而L~2~正则化（ 权重衰减通常也被称为L~2~正则化），其实就是给**W**增加一个衰减因子，衰减因子会在梯度下降法时每更新一次权重，就会衰减一次，通过这样的方法让**W**逐渐的减小，从而控制高次项对函数的贡献，从而降低模型容量达到了正则化的目的，减少了神经网络的过拟合。==这就是权重衰减

*前面一大堆就是解释为什么权重的衰减能达到正则化*

ok，明白了权重衰减的原理，那么怎么做到权重衰减呢？

首先我们需要一个衡量权重大小的方式，一种简单的方法是通过线性函数 𝑓(𝐱)=𝐰⊤𝐱中的权重向量的某个范数来度量，范数越小，说明权重越小。这里我们使用L~2~范数来作为度量，即保证‖𝐰‖^2^越小

 保证‖𝐰‖^2^小， 最常用方法是将其范数作为惩罚项加到最小化损失的问题中。

将原来的训练目标***最小化训练标签上的预测损失***， 调整为***最小化预测损失和惩罚项之和***
$$
min(L(\bold w,b)+\frac{\lambda}{2}||\bold w||^2)
$$
当λ = 0时恢复原来的损失函数，λ > 0时限制了‖𝐰‖^2^大小，这里除以2为了求导时方便计算

调整之后权重**w**如何进行衰减呢

现有损失函数和权重更新公式为
$$
L(\bold w,b)\\
\bold w = \bold w - \eta \nabla_{\bold w}L(\bold w)
$$
其中w为权重η为学习率，▽~w~L(w)为梯度，就很普通的权重公式

那么在引入‖𝐰‖^2^作惩罚后有损失函数L^'^(w,b)
$$
L^{'} = L(\bold w,b)+\frac{\lambda}{2}||\bold w||^2\\
L^{'} = L(\bold w,b)+\frac{\lambda}{2}\bold w^{T}\bold w
$$
权重更新有
$$
\bold w = \bold w - \eta \nabla_{\bold w}L^{'}(\bold w)\\
\quad\quad\quad\quad\quad\quad=\bold w = \bold w - \eta \nabla_{\bold w}L(\bold w) - \eta \lambda\bold w\\
\quad\quad\quad\quad=(1-\eta\lambda)\bold w - \eta \nabla_{\bold w}L(\bold w)
$$
将两个更新权重的式子进行比较（第一行是为加入L~2~范数的权重更新，第二行是加入L~2~~2范数的权重更新）
$$
\bold w = \bold w - \eta \nabla_{\bold w}L(\bold w)\\
\bold w =(1-\eta\lambda)\bold w - \eta \nabla_{\bold w}L(\bold w)
$$
两个权重更新的表达式中只有一个差别，就是正则化后权重更新的表达式中权重**w**的系数为(1 - ηλ)

学习率η和λ两个超参数相乘小于1大于0时，每一次权重更新，即每一步梯度下降法的学习**w**权重都要进行一步缩小然后再减去学习后下降的梯度值，从此就可以看出w衰减在了哪里，就是衰减在前面的系数上

*终于搞明白啦！再次感谢王老师👑🪵*

### 权重衰减的实现：

以高维线性回归的例子实现权重衰减

以该函数为例：

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220617192000574.png" alt="image-20220617192000574" style="zoom: 50%;" />

标准权重为0.01向量，偏置为0.05，并对最后的输出添加噪音ε生成人工数据集

~~~py
%matplotlib inline
import torch
from torch import nn
from d2l import torch as d2l

n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5
#n_train - 训练集只有20个样本,训练数据越小模型越容易产生过拟合
#num_inputs - 200特征的维度,权重向量的长度

true_w, true_b = torch.ones((num_inputs, 1)) * 0.01, 0.05
#高维函数的真实标准权重

train_data = d2l.synthetic_data(true_w, true_b, n_train)
#synthetic_data() - 生成数据集函数,见(线性神经网络2.0-线性回归从零开始-生成数据集-生成数据集函数)
train_iter = d2l.load_array(train_data, batch_size)
#load_array() - 读取内存中的数组生成迭代器
test_data = d2l.synthetic_data(true_w, true_b, n_test)
test_iter = d2l.load_array(test_data, batch_size, is_train=False)
#以上生成训练数据集,训练数据集迭代器、测试数据集,测试数据集迭代器
~~~

#### 从零开始实现：

**首先初始化模型参：**

~~~py
def init_params():
    w = torch.normal(0, 1, size=(num_inputs, 1), requires_grad=True)
    b = torch.zeros(1, requires_grad=True)
    return [w, b]
~~~

初始化模型参数函数，返回参数列表，参数初始化都用函数初始了，相比之前的从零开始实现还是变量的直接赋值。也许这就是成长吧，哈哈

==**定义L~2~范数惩罚：**==

多了新的东西就是下面的这个函数，也是权重的衰减的核心。需要将它放在目标函数里面，保证目标函数较小

~~~py
def l2_penalty(w):
    return torch.sum(w.pow(2)) / 2
#向量的二范数,向量各元素求平方后并将它们求和,常数系数1/2方便求导
#这里没有加超参数λ
~~~

**训练：**

将模型拟合训练数据集，并在测试数据集上进行评估

和第三节相比线性网络和平方损失没有变化， 所以我们通过**d2l.linreg**（定义模型）和**d2l.squared_loss**（最小二乘法损失）导入它们

 唯一的变化是损失现在包括了惩罚项

~~~py
def train(lambd): #定义训练函数,参数为惩罚项的超参数λ,通过改变λ的值来观察训练误差来体现权重衰减的影响
    
    w, b = init_params() #初始化模型参数
    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss
    #构建模型,导入损失函数
    
    num_epochs, lr = 100, 0.003
    #训练集训练迭代次数和,梯度下降学习率
    
    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',
                            xlim=[5, num_epochs], legend=['train', 'test'])
    #训练误差(train)和泛化误差(test)可视化
    
    for epoch in range(num_epochs):
        for X, y in train_iter: 
            
            #增加了L2范数惩罚项，
            #广播机制使l2_penalty(w)成为一个长度为batch_size的向量
            l = loss(net(X), y) + lambd * l2_penalty(w) #更改训练目标
            
            l.sum().backward() #梯度下降
            d2l.sgd([w, b], lr, batch_size) #参数优化更新
            
            
        if (epoch + 1) % 5 == 0: #每迭代5次更新训练可视化图像
            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),
                                     d2l.evaluate_loss(net, test_iter, loss)))
    print('w的L2范数是：', torch.norm(w).item())
~~~

- 当λ = 0时，禁用权重衰减，忽略正则化直接训练

~~~py
train(lambd=0)

w的L2范数是： 13.756134986877441
~~~

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220617195042005.png" alt="image-20220617195042005" style="zoom:50%;" />

睡着训练次数的增加，训练误差有明显的减小，但测试误差并没有减少，这意味着产生严重的过拟合

- 当λ = 10时，使用权重衰减

~~~py
train_concise(10)

w的L2范数： 0.1586313098669052
~~~

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220617195241624.png" alt="image-20220617195241624" style="zoom:50%;" />

与上图相比，训练误差明显增大，但测试误差减小，这正是我们期望正则化得到的效果

#### 简洁实现：

上面从零开始的实现，我们更改了训练目标，在损失函数后面添加了惩罚项。

而深度学习框架为了更方便使用权重衰减，将权重衰减集成到了优化算法中，从而可以与任何损失函数结合使用

其原理就是将权重更新参数化简到下列公式：
$$
\bold w =(1-\eta\lambda)\bold w - \eta \nabla_{\bold w}L(\bold w)
$$
即每次更新权重时直接代入λ即可，这样还能保证预测目标损失求导时少算点

所以对于深度学习框架一般都在优化算法程序提供一个==**weight_decay**==字段可供选择

下面看一下代码：

~~~py
def train_concise(wd):#wd - 权重衰减超参数,公式中的λ
    
    net = nn.Sequential(nn.Linear(num_inputs, 1)) #定义模型
    for param in net.parameters():
        param.data.normal_() #初始化模型参数
    loss = nn.MSELoss(reduction='none') #定义损失函数
    
    num_epochs, lr = 100, 0.003 #迭代次数,学习率
    
    trainer = torch.optim.SGD([
        {"params":net[0].weight,'weight_decay': wd},
        {"params":net[0].bias}], lr=lr)
    #优化算法中,将权重中的衰减参数设置为wd
    #偏置参数没有衰减
    
    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',
                            xlim=[5, num_epochs], legend=['train', 'test'])
    #训练误差(train)和泛化误差(test)可视化
    
    for epoch in range(num_epochs):
        for X, y in train_iter:
            trainer.zero_grad()
            l = loss(net(X), y)
            l.mean().backward()
            trainer.step()
    #普通的训练过程,权重衰减直接在优化算法SGD中自动更新        
           
        if (epoch + 1) % 5 == 0:
            animator.add(epoch + 1,
                         (d2l.evaluate_loss(net, train_iter, loss),
                          d2l.evaluate_loss(net, test_iter, loss)))
    print('w的L2范数：', net[0].weight.norm().item())
~~~

调用函数训练即可，其结果与从零实现区别不大，随着对wd的设置，权重的二范数逐渐减小

但相比从零实现，它们运行得更快，且更容易实现。对于更复杂的问题，这一好处将更加明显😸
