## 线性回归的简洁实现

### 定义模型：

模型定义利用torch中nn模组

~~~py
# nn是神经网络的缩写
from torch import nn

net = nn.Sequential(nn.Linear(2, 1))
#net为构建的线性回归模型
~~~

#### torch.nn.Linear

这是一个类，线性类，神经网络的线性层，用于对输入数据进行线性变换

torch.nn.Linear()是该类的构造函数

在提一嘴

~~~py
import torch
from torch import nn
#有什么区别

#其实很简单,如果只是import torch,那么在使用神经网络的线性类时,前面需要加上torch
torch.nn.Linear(2,1)
#但将nn模组指名是从torch中导出的,即from torch import nn
nn.Linear(2,1)
#可以省略前面的前缀直接调用
~~~

构造函数nn.Linear()实例化时有两个参数

- **in_features** -- 输入神经元的个数，这里为2
- **out_features** -- 输出神经元的个数，输出单个标量，因此为1。

#### weight和bias

所有nn.Module的子类，形如nn.XXX的层，都会在实例化的同时随机生成weight和bias的初始值。

所以实例化之后，不需要定义权重和偏置，其中weight是必然会生成的，bias可以根据需求控制是否生成

比如nn.Linear类中，有参数bias，默认 bias = True。如果我们希望不拟合常量b

在实例化时nn.Linear(bias= False)即可

#### torch.nn.Sequential

Sequential类是一个顺序容器，其中模块的添加顺序与在构造函数中传递模块时的顺序相同。

 Sequential类可以将多个层串联在一起。如果有多个层时，对给定的输入数据，Sequential实例将数据传入到第一层， 然后将第一层的输出作为第二层的输入，以此类推。 

当然线性模型只有一层，但以后更复杂的模型肯定有很多层，可以用这个类做多层的神经网络。

这里使用这个类只是方便以后学习自然衔接😸

### 初始化模型：

建立完模型后，还需要初始化模型参数。

从零开始时参数是设置的变量然后进行随机赋值，而深度学习框架直接选取模型对应的层数通过weight.data和bias.data方法访问参数，然后调用normal\_() 和fill_()方法初始化

由于net使用了Sequential类，是一个多层的神经网络，但在定义时只有一层，所以net[0]就直接表示那一层

~~~py
#net = nn.Sequential(nn.Linear(2, 1))
net[0].weight.data.normal_(0, 0.01) #从均值为0、标准差为0.01的正态分布中随机采样
net[0].bias.data.fill_(0) #偏置参数将初始化为零
~~~

#### torch.nn.Parameter

一个张量类，将被视为模块参数。

torch.nn.Parameter继承于torch.Tensor，tensor的子类。

其作用将一个不可训练的类型为Tensor的参数转化为可训练的类型为parameter的参数，并将这个参数绑定到module里面，成为module中可训练的参数。

torch.nn.Parameter(Tensor data, bool requires_grad)

- **data** -- 传入Tensor类型参数
- **requires_grad** -- 默认值为True，表示可训练，False表示不可训练

但书上使用的是.weight.data来访问的参数，有的时候带data有的时候不带data....让我很费解，就百度了一下真有好哥哥出来解释，真好😸。

但解释的感觉也模棱两可，以参数weight为例解释一下

**.weight.data：**得到的是一个张量（朴实无华），可以对这个张量进行张量操作，不可求导，不能进行训练
**.weight：**得到的是一个parameter的变量（也是张量），可以计算梯度，能进行训练

两个变量得到的数值是一样的，但使用时有区别。不求导初始化时用上面的，训练求梯度时用下面的

#### normal\_()和fill_()

normal_()将张量元素数值正态分布

~~~py
import torch

x = torch.zeros(3,4)

x.normal_(0,0.01) #均值0.01,标准差为1的正态随机分布

tensor([[-0.0194,  0.0057, -0.0147,  0.0093],
        [ 0.0011, -0.0041, -0.0146,  0.0142],
        [-0.0027,  0.0074,  0.0055, -0.0127]])
~~~

fiil_()对张量数值进行填充设置的值

~~~py
x.fill_(3)

tensor([[3., 3., 3., 3.],
        [3., 3., 3., 3.],
        [3., 3., 3., 3.]])
~~~

这两个函数常常用在神经网络模型参数的初始化中

### 定义损失函数：

#### torch.nn.MSELoss

MSELoss类，用于衡量输入x后的输出和目标y中每个元素之间的均方误差的标准。

也称为平方𝐿~2~范数， 默认情况下，它返回所有样本损失的平均值。

~~~py
loss = nn.MSELoss()
~~~

### 定义优化算法：

小批量随机梯度下降算法是一种很好用的优化算法，深度学习框架肯定有做好现成的了，所以只要知道如何使用就行了。

#### torch.optim.SGD()

torch.optim是一个用于实现优化算法的包，可以用这个包构造优化器optimizer

optimizer用于根据已得到的梯度来更新网络参数。

构造优化器需要传入一个包含要学习参数的迭代器，此外还有一些学习策略参数如学习率等。

这里使用的是优化器SGD

实例化一个SGD（小批量随机梯度下降）时，只要指定优化的参数以及优化算法所需的超参数字典即可

- **params ** -- 待优化参数的迭代类对象或者是定义了参数组的dict
- **lr** --  (float) 学习率

当然还有其他参数，不过现在用不到

指定优化的参数可通过net.parameters()获得。

net**.parameters()**是一个方法，它能将这个模型下的参数作为生成器进行返回，生成器是可迭代的

对于小批量随机梯度下降只需要设置一个超参数lr值（学习率），这里设置为0.03。

~~~py
trainer = torch.optim.SGD(net.parameters(), lr=0.03)
~~~

调用优化函数后，trainer就是一个optimizer（优化器），可以调用优化器中的方法

### 训练：

使用深度学习框架的API可以很简单的实现线性回归，当需要更复杂的模型，高级的API优势将更大，因为不用手动去构建模型和优化算法，这样不仅能保证正确性还能提高效率，毕竟人家都是优化好之后给你封装好了，自己真没必要去实现了。

从零实现的训练过程：

1. 完整遍历一遍数据集，不停地从中获取一个小批量
2. 对每个小批量使用net模型生成预测并计算损失loss
3. 通过损失函数计算梯度
4. 根据优化算法利用梯度更新模型参数

~~~py
num_epochs = 3 #数据集遍历次数

for epoch in range(num_epochs): #遍历3次
    for X, y in data_iter: #每次选取一个小批量
        l = loss(net(X) ,y) #调用损失函数计算损失
        trainer.zero_grad() #每次更新迭代时,清空上次计算梯度
        l.backward() #计算该批量下损失函数的梯度
        trainer.step() #梯度下降,参数更新
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')
~~~

#### optimizer.step()

在用pytorch训练模型时，在遍历epochs的过程中依次用到**optimizer.zero_grad()**,**loss.backward()**和**optimizer.step()**三个函数成套出现

==这三个函数的作用是先将梯度归零（optimizer.zero_grad()），然后反向传播计算得到每个参数的梯度值（loss.backward()），最后通过梯度下降执行一步参数更新（optimizer.step()）==

而step()函数的作用是执行一次优化步骤，通过对应生成器的梯度下降法来更新参数的值。

（本例使用的是SGD生成器）

因为梯度下降是基于梯度的，所以在执行optimizer.step()函数前应先执行loss.backward()函数来计算梯度。

⚠️注意：optimizer只负责通过梯度下降进行优化，而不负责产生梯度，梯度只有调用tensor.backward()方法产生的

以上就是线性回归的简洁实现，从训练中就不难发现，使用深度学习框架中已经优化好的API来写真的既好看又舒服😸

## softmax回归

### 分类问题：

回归可以预测数值多少，但我们对分类问题也感兴趣

回归是估计一个连续值，单个输出；分类是预测一个离散的类别，通常是多个输出

以简单的问题一个类别识别入手，给了一堆猫猫😸图（其中有可能是猫有可能不是猫），我们需要一个模型来识别这些图片判断这张图片是不是一个猫

那么判断猫猫🐱前需要一个基准，什么为基准呢？人的判断为基准！

我们人脑就有一种天赋，可以把各种各样类型的猫赋予一种边界，能够清晰的划定出来，超出这个边界我们就认为它不是🐱

概括的说，人脑中就有一个模型，可以把猫的边界准确的圈定出来。

但这个模型是模糊的，我们需要在神经网络里面具体化，并且要保证利用神经网络设计的模型尽可能的与人脑中的模型一致

但我们人脑这个模型到底什么样子，我们根本不清楚，也描述不出来

但幸运的是，我们可以根据特征去大量的识别猫猫🐱，然后根据这些正确的样本数据去训练优化我们的模型，使我们设计的模型根据训练集的输入特征得到预测输出不断逼近训练集的标签

这就是神经网络要解决的分类问题

我们利用模型得到的预测输出和训练集的标签是否接近，这个量如何度量，和线性回归一样，我们可以利用损失函数来进行度量，预测值的损失函数很简单是最小二乘法，而对于分类问题的损失我们可以利用交叉熵来度量

### 损失函数：

#### 最大似然估计：

在讲一下softmax回归的损失函数时先讲一下似然估计

**似然估计**利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值

*先看一个例子吧，很多博客都举得这个例子我觉得说的很好，这里copy和稍微魔改一下* 🐶

有一个箱子里面黑白两球，数目不知，黑白球比例未知，每次从箱子里面拿球，设从箱子里拿出的白球概率为p，因为只有两种球，所以黑球的概率为1-p

每抽一个球，在记录颜色之后，把抽出的球放回了箱子里并摇匀，**所以每次抽出来的球的颜色服从同一独立分布。**

十次抽取后，七次是白球的，三次为黑球

将该事件的概率记为P(样本结果|Model)。

如果第一次抽象的结果记为x~1~，第二次抽样的结果记为x2....那么样本结果为(x~1~,x~2~.....,x~10~)。

可以得到如下表达式：

P(样本结果|Model)

= P(x~1~,x~2~,…,x~10~|Model)

= P(x~1~|Mel)P(x~2~|M)…P(x~10~|M)

= p^7^ (1-p)^3^.

故根据抽样结果，有了观察样本结果出现的概率表达式了。那么我们要求的模型的参数，也就是求的式中的p。

**但我们本能会认为这里的P = 70/100**，但原因是什么❓

实际上P是有无数多种分布的。如下：

| P(白球概率) | 1-P（黑球概率） |
| :---------: | :-------------: |
|     0.7     |       0.3       |

| P(白球概率) | 1-P（黑球概率） |
| :---------: | :-------------: |
|     0.5     |       0.5       |

| P(白球概率) | 1-P（黑球概率） |
| :---------: | :-------------: |
|     0.8     |       0.2       |

等等......

那么将以上数值代入 p^7^(1-p)^3^

分别为 

(0.7)^7^(0.3)^3^=0.0022235661

(0.5)^7^(0.5)^3^=0.0009765625

(0.8)^7^(0.2)^3^=0.0016777216

当然分布还有其他无数种，所以有无数种分布可以选择，极大似然估计应该按照什么原则去选取这个分布呢？

==既然事情已经发生了，为什么不让这个出现的结果的可能性最大呢？==**这也就是最大似然估计的核心。**

所以我们的感觉并没有错！

对于上面的事件概率模型P(样本结果|Model)可作为函数，用数学符号表示：
$$
P(x|θ)
$$
对于该函数x为样本，θ为模型参数

当θ已知，x为变量，函数为概率函数

当x已知，θ为变量，函数为似然函数

似然函数的函数意义，当θ取某个值时，x这个样本点的概率为多少

这个样本事件已经发生了，所以要尽可能的使这个结果发生的可能性最大，这个概率越大说明这个模型越准确越拟合，所以问题转换为了函数在θ为何值时，函数值最大

到这里是不是和什么东西有挂钩！！！？？？？

没错！分类神经网络

对于我们这个识别猫猫😸图片的模型，我们的训练集是一系列样本数据(x~1~,x~2~...x~i~...,x~n~)，这是一个很简单的模型，识别也很简单，这张图片要么是猫，要么不是猫，我们用1代表是猫，0代表不是猫。

x ∈{0,1}

哈哈，是不是有点像上面的白球黑球啦

对于这个样本中一个样本点的似然函数就有了P(x~i~|θ)，θ参数就是模型网络中的权重W和偏置b

所以进一步概括为P(x|W,b)

但我们希望能写出似然函数的表达式，可是对于分类问题的特征输入利用权重和偏置是如何得到输出我们是不关心的，或者可以说不需要去想底层实现的逻辑，我们只需要知道根据特征值输入到网络中能得到一个预测的输出，这个输出记为y，输出这个值会由网络处理把它转换为一个概率，比如这张图是猫猫🐱的概率为y，不是猫猫的概率为1-y

这样我们可以把似然函数看作为P(x|y)，其中y的产生是依赖于权重W和偏置b得

其中y~i~为第i个样本点输入的特征是猫猫图的概率，1-y~i~不是猫猫图的概率，x~i~ ∈{0,1}

**类比上面黑白球的似然函数构建**👻

故对于一个样本点有P(x~i~|y~i~)  =  y~i~^xi^(1-y)^1-xi^
$$
P(x_i|y_i)  =  y_i^{x_i}(1-y_i)^{1-x_i}=\begin{cases} y_i，x_i=1\\ 1-y_i， x_i=0\end{cases}
$$
整个样本有
$$
\prod_{i=1}^{n}{P(x_i|y_i)}\\=\prod_{i=1}^{n}{ y_i^{x_i}(1-y_i)^{1-x_i}}
$$
⚠️弹幕好哥哥up主说，二项分布求概率应该有系数C(K,n)，不过训练数据集确定后这个系数为常数，可以不考虑

但因为这个似然函数是连乘，我们更喜欢连加

将该样本，其实可以叫做训练集啦，可以将训练集的这个似然函数取对数转换为连加

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220607222747760.png" alt="image-20220607222747760" style="zoom:25%;" />

ok，这样顺眼多了，我们的目的是什么？我们的目的是求极大似然值，当这个似然值足大，说明我们这个模型的参数就越精确，上面是这个训练集的极大似然值，我们取了对数转换为了连加，而连加是不改变数据的单调性的。

所以即求下面上述式子下面两行的最大值，但在应用时我们更习惯取最小，只需加个符号，就能得到类似于书上的公式

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220607222701938.png" alt="image-20220607222701938" style="zoom:25%;" />

这就是利用似然估计得到的损失函数，但弹幕好哥哥都说这个是交叉熵....这又是什么高级的新名词、这不是利用最大似然估计推出来的损失函数吗，保证这个值越小模型越精确。下面解释一下什么是交叉熵！

*感谢up主王木头学科学的讲解，这个例子也是引用的他的，然后加上我自己的理解明白了神经网络如何利用似然估计做损失函数*🐶

#### 交叉熵：

交叉熵损失是分类问题中最常用的损失之一，想要理解交叉熵还需要通过信息论基础来了解几个概念

##### 信息量：

说实话，我理解起来很抽象。概括的说信息量是描述一个事件从原来的不确定变得确定，它的难度有多大。

信息量越大，这个事件发生的难度就越高。

比如计算机中有8位二进制数，取0或者取1，这8位随机确定一个数比如74H，记该事件记为A，则事件A的概率为

1/2^8^，4位二进制数确定一个数为4H，该事件记为B，那么事件B概率为1/2^4^

相较两个事件，哪一个事件发生的难度比较大？自然是事件A，所以可以说事件A的信息量大

但做事不能凭感觉，需要有东西来量化数据中的信息内容，而这个量化的数，就是信息量

在最早的初步设想，消息数的对数来定义为信息量。比如信源有m种消息，且每个消息是以相等可能产生的，则该信源的信息量可表示为I=logm

事件A中8位二进制数确定74H，信源有2^8^种结果，74H只是其中的一种，那么信息量为log(2^8^) = 8，同理，事件B的信息量可得log(2^4^) = 4

但随着信息量的完善，香浓指出信源给出的符号是随机的，所以我们更倾向于用信源产生某个结果的概率来表示信息量，所以此时信息量是概率的函数，可以通过以下方程得到
$$
H(x_i) = -\log_2p_i
$$
其中p~i~表示信源不同种类符号x~i~发生的概率，i= 1，2，…，n

还是以事件A为例子，代入公式可得H(74H) = -log(1/2^8^) = 8，哈哈结果与刚刚一样😸，这样是不是有点显得多此一举？

但与刚刚相比的含义已经变了，刚刚的初步设想是以信源的总数作为真数，但实际应用并不好运用，因为复杂的事件的一般不是单纯的从总数中取一个。

事件的发生我们通常得到是一个概率，而现在套用这个公式根据事件发生的概率就能得到信息量的大小，根据这个公式也可以看出，事件的概率越小，所得信息量越大，这个事件发生的难度就越高，同样是符合逻辑的。

⚠️还有一点需要补充的是，信息量函数对数的底数可以取任何值，只是不同底数对应的信息量单位不同，这里的底数为2则单位为bit，因为根据事件A的实际情况每位只能取0或1这两种情况，故以2为底得到的信息量是整数，可以很好表示所以才选的2。（==以底数e得到的信息量，其单位为纳特nat，1nat 约等于 1.44bit==）

❓最后抬杠问一下为什么信息量用这种对数表示？这个玄而又玄的东西是怎么度量出来的呢？

我一开始也这样想钻了很久的牛角尖，但终于想明白了，没有为什么，默认会用就行

*因为这就是定义，人为的定义，先辈的智慧，它不像定律一样通过大量的事实证明这个规律是客观存在的，定义是人为的描述，并且这个描述是符合逻辑的，能够自洽具有完整的体系，我们只需要默认学习即可。假设还有一个平行世界，那么在那个世界中的信息量定义可能又是不同的，但它的逻辑和含义应该是一样的，只是人们定义的形式不一样罢了。所以在以后的学习，遇到新的知识新的定义就不用太钻牛角尖啦，不要在问为什么公式是这样而不是那样，其实有时可能并没有很多科学依据，只是人为定义的形式符合逻辑罢了。*

##### 熵：

信息量可以理解为一个事件从原来的不确定变得确定难度有多大，熵也类似，但是熵不再是衡量某一个具体的事件，它是衡量整个系统里的所有事件。

一个系统由确定到不确定其难度有多大，可以通过熵来反映

引入书中的概念，有一个概率系统P，我们求它的熵，这个熵即对这个系统求信息量的期望，由期望公式可得

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220608170824978.png" alt="image-20220608170824978" style="zoom: 50%;" />

把系统中所有可能发生的事件，信息量求出来和对应事件发生的概率相乘，最后把所有事件加起来，得到的就是这个系统的熵

所以熵就可以对整体的概率模型进行一个衡量，衡量的结果就能反应出这个模型的不确定程度，或者说混乱程度

好了，有了熵就可以用熵来比较两个模型，最简单的比较就是将这两个模型的熵算出来然后进行比较

但这种方法并不是那么简单，因为人所认知的识别概率模型并不是那么好定义的，没有办法直接求熵

所以需要另外一个概念，相对熵——KL散度

##### 相对熵：

相对熵，相对相对说明就不是一个概率系统啦，而是两个系统设有两个系统P和Q，概率分布分别为![[公式]](https://www.zhihu.com/equation?tex=p%28x%29%2Cq%28x%29),事件的样本即分布律中的x是一样的。

则![[公式]](https://www.zhihu.com/equation?tex=p)相对![[公式]](https://www.zhihu.com/equation?tex=q)的相对熵为：
$$
D_{KL}(P||Q) = \sum_{i=1}^{m}p_i((-\log_2qi)-(-\log_2pi))
$$


以P为基准，在系统Q中的信息量减去它对应P中的信息量这个差值最后求整体期望就是相对熵

如果说Q这个模型和P模型是完全相等的，则KL散度为0，所以说如果等于0的话那就说明P和Q这两个模型是完全一样的，如果不为0这说明这两个模型是有差别的

将上式继续展开
$$
D_{KL}(P||Q) = \sum_{i=1}^{m}p_i(-\log_2qi)-\sum_{i=1}^{m}p_i(-\log_2pi))
$$
对于上面这个式子的后半段就是系统P的熵，符合前面熵的定义，这个熵是恒定的

而前面部分
$$
H(p,q) = \sum_{i=1}^{m}p_i(-\log_2qi)
$$
就是***交叉熵***,😿真不容易，交叉熵终于出现了，用H(P,Q)来表达

还没有结束，KL散度等于0时Q和P是最接近的，在相对熵中后半部分，P的熵一定是一个大于0的数

同理交叉熵也是大于0的数，因为去掉p~i~概率就是Q的信息量，信息量是正数，正数乘一个概率也是正数

扯多啦，不是问题关键

由==**吉布斯不等式**==可得，在KL散度中前半部分的交叉熵是一定大于后半部分系统P的熵的，所以KL散度是一定大于0的，KL散度越小说明Q和P越像，为了保证KL散度小，而P系统的熵是一定的，就要保证交叉熵越小

所以交叉熵这个式子，本身就可以作为损失函数

交叉熵越小，就说明这两个概率模型越接近

回归到神经网络中，如何利用交叉熵呢？

交叉熵定义：
$$
H(p,q) = \sum_{i=1}^{m}p_i(-\log_2qi)
$$
根据定义，我们只需要将p~i~和q~i~对应于神经网络中的什么进行替换？

我们比较的概率模型是人脑的模型，是人脑识别后的判断，是训练集中的标签值

以识别猫猫图🐱为例，标签值x~i~就两种情况0(不是猫)，1(是猫)，所以将式子中的p~i~换成x~i~

那么对比的q~i~能直接换成y~i~吗？

x~i~只有两种情况，要么是猫要么不是猫，但y~i~始终是在说明这个概率是有多像猫，并没有去判断不像猫的概率

而这个公式是需要将两个值对应起来的，比如x~i~为1时，q~i~为多像猫的概率，x~i~为0时，q~i~为多不像猫的概率，所以如果需要用y~i~来替换的话，还需要继续展开，整理可得
$$
H = -\sum_{i=1}^{2}(x_i\log_2y_i + (1-x_i)\log_2(1-y_i))
$$
i = 1,2，x~1~ = 1，x~2~ = 0

⚠️注意：虽然这里有连加的符号，但这只是说明一张图片的两种情况，每一个输入的图片都要计算一次这个，在多类分类问题中就只是损失函数的一个分量

这样写是不是很熟悉？这个就是用极大似然估计法推出的式子

#### 交叉熵和最大似然估计的区别：

绕了那么大的圈，结果这个交叉熵和最大似然估计推出来的式子是一样的，但只是形式上一样

极大似然估计法中的log是因为我们习惯去做连加不习惯连乘而改的式子的形式，以哪个数为底都是无所谓的

但交叉熵中的log是写在定义中的，以2为底能代表最后计算出的单位是比特

所以说从数学角度看，形式是一样的，但以物理角度来说一个没有量纲，一个有量纲

其次，求最小值时，最大似然估计按理来说是求最大值，但为了符合习惯强行加负号转换为了最小值，而交叉熵就是写在定义中的

所以交叉熵作为分类问题的损失是更合理的

### softmax运算：

前面举得都是单类识别问题，但实际应用我们不可能只辨别一类，一般都是多分类问题

假设我们有4个特征，有3个识别的类别，我们在网络架构时只需保证每个输出都有对应它自己的仿射函数即可

为每个输入计算三个==*未规范化的预测*==（logit）：o~1~、o~2~、o~3~
$$
o_1 = x_1w_{11}+x_2w_{12}+x_3w_{13}+x_4w_{14}+b_1\\
o_2 = x_1w_{21}+x_2w_{22}+x_3w_{23}+x_4w_{24}+b_2\\
o_3 = x_1w_{31}+x_2w_{32}+x_3w_{43}+x_4w_{34}+b_3\\
$$
多类识别相较于单类识别，权重不在是一个向量，而是一个矩阵；偏置不再是一个标量，而是一个向量

输出也不再是一个标量，同样是一个向量

现在有了网络架构，那么这个网络的输出是什么？

在损失函数中说，输出是这个类别多像的概率

“概率”本身就有两个限制条件，如果输出的为概率一方面我们要保证输出的这些预测值的总和为1，另一方面，根据输入不同，所以输出有可能为负数，而概率是不能为负的

所以需要将这些预测值进行规范化，而如何进行规范化？即——**softmax运算**

softmax函数可以将未规范的预测变换为非负且总和为1，同时模型保持可求导

首先对每个未规范化的预测求幂，这样可以确保输出非负。 为了确保最终输出的总和为1，我们再对每个求幂后的结果除以它们的总和。如下式：

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220609164812093.png" alt="image-20220609164812093" style="zoom: 50%;" />

其中每个分量有

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220609164841214.png" alt="image-20220609164841214" style="zoom:50%;" />

这样对于所有的𝑗总有0≤𝑦̂ ~𝑗~≤1。 因此，𝐲̂ 可以视为一个正确的概率分布。

这样经过规范化后的输出就可以利用损失函数去做损失，但最终的输出我们要的并不是这个类别多像的概率，而是形同标签一样具体的值，这个方法可以通过编码进行实现。

以猫猫图🐱单类识别为例，如果输出的y~i~大于0.5，我们可以说第i张图是像猫的

🐱、🐔、🐶三类识别，最终输出为[ 0.1, 0.8, 0.1 ]\(经过规范化)，我们可以利用**one-hot**编码（见预备知识2.0—处理缺省值—pandas.get_dummies()处有解释）将向量中分量最大的概率设为1，其他设为0即[ 0, 1, 0 ]来表示预测的结果为🐔（100为🐱、001为🐶），这些概率分量也有个好听的名字叫做置信度，置信度越大说明这个类的可能性越高

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220609171110902.png" alt="image-20220609171110902" style="zoom:45%;" />

并且我们建立的模型最后的输出中最大的置信度相比其他要足够大，即识别正确类的置信度要远远大于非正确类的置信度，这又是为什么呢？在举个例子，假设输出有[ 0.5, 0.01, 0.49 ]，虽然置信度最大的是🐱，但我们能说是猫？因为狗的置信度和猫的置信度是非常接近的，这个界限是模糊的，所以需要将正确类别的置信度相较于其他非正确类别置信度有更大的余量！

以数学形式表示，我们希望正确类别相较于非正确类的差大于某个阈值，这就要看分类模型的要求啦

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220609171047153.png" alt="image-20220609171047153" style="zoom:45%;" />

这样就能保证正确的类和非正确的类能够拉开距离

扯远了....🙉，总之softmax回归就是将线性模型的输出规范化成概率，可以进行损失

虽然softmax函数是一个非线性函数，但softmax回归的输出仍是由线性模型的网络架构得到的，只是在之后通过softmax函数规范化，所以softmax回归仍是一个线性模型

##  softmax回归实现

### 图像分类数据集：

在实现softmax回归时需要训练集进行训练

MNIST数据集 ——手写数字识别

但作为基准数据集过于简单。 我们将使用类似但更复杂的**Fashion-MNIST数据集**

该数据集包含10个类别，都是些衣服之类的

读取数据集：

~~~py
import torch
import torchvision #pytorch对于计算机视觉一些模型实现的库
from torch.utils import data #模块内有处理数据小批量的函数
from torchvision import transforms 
#将模块中对数据操作处理中的模组导入,调用时不用加前缀torchvision.
from d2l import torch as d2l

def load_data_fashion_mnist(batch_size, resize=None): #读取数据集中的批量大小,数据集图片大小设置
    """下载Fashion-MNIST数据集，然后将其加载到内存中的函数"""
    trans = [transforms.ToTensor()] #将图片格式转换为张量,格式设置变量
    
    if resize: #如果该值为真,根据该值设置数据集每张图片的大小,单位为像素,图片形状为正方形
        trans.insert(0, transforms.Resize(resize))
    
    trans = transforms.Compose(trans) #?意义不明,好像是将多个步骤何为一体
    
    mnist_train = torchvision.datasets.FashionMNIST(
        root="../data", train=True, transform=trans, download=True)
    #将FashionMNIST数据集下载到本地文件
    #root="../data" -- 下载到上级目录的data中
    #train=True -- 下载数据为训练集
    #transform=trans -- 下载后图片格式转换为张量
    #download=True -- 默认从网络中下载
    
    mnist_test = torchvision.datasets.FashionMNIST(
        root="../data", train=False, transform=trans, download=True)
    #与训练集一样的文件,但不参与训练,用于对照判断模型的好坏
    #download=True -- 下载数据不进行训练
        
    return (data.DataLoader(mnist_train, batch_size, shuffle=True,
                            num_workers=get_dataloader_workers()),
            #调用data.DataLoade()函数,返回大小batch_size的训练数据生成迭代器
            #mnist_train -- 相当于一个数据库,从其中抽取数据
            #batch_size -- 抽取数据的大小
            #shuffle=True -- 抽取随机
            #num_workers=get_dataloader_workers() 
            # -- 进程工作数,以函数设置数量,若要调整数量时直接对函数进行重新设置
    """
    def get_dataloader_workers():  #使用4个进程来读取数据
    	return 4 #因为图片在硬盘上,为了加速读取每一次用4个进程来进行读取操作
    """     
            data.DataLoader(mnist_test, batch_size, shuffle=False,
                            num_workers=get_dataloader_workers())
            #shuffle=False -- 按数据集顺序抽取
            #其余同上
           )
~~~

ok，现在已经准备好使用Fashion-MNIST数据集，下面实现softmax回归

*在实现之前，先等等。课本上有两种实现方式，从零开始和简洁实现，上个星期的线性回归从零实现差点把我送走，从零实现都是李沐老师一点一点实现的，就用于教学，有的地方实现效率很低，并且有时不严谨还有错误，简洁实现就是直接调用深度框架里人家写好的函数算法，都是经历过好几代优化好的，只要知道那几个接口怎么用就可，从零实现确实能帮我梳理一下原理过程并且加深pytorch的使用，但说实话这个过程很痛苦，偷个懒，只学简洁实现啦......*😿*，不过简洁实现的前提还是要把原理学好的，不然也不清楚为什么要这样调用函数。自我感觉softmax回归的原理还可以 ，感觉上手简洁实现也没问题啦！*

*这才说明我入门了啊，都懂的取舍了，刚开始学的时候，什么都看、什么都学，感觉什么都重要，并且学了也不知道学这个东西的意义是什么，现在不一样啦！学会取舍也是一种进步*🐻‍❄️

~~~py
import torch
from torch import nn #神经网络模块
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) 
#获取小批量训练集和对照集,批量大小为256,不改变图像大小
~~~

### 初始化模型参数：

~~~py
# PyTorch不会隐式地调整输入的形状。因此，
# 我们在线性层前定义了展平层（flatten），来调整网络输入的形状
net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))

def init_weights(m): 
    if type(m) == nn.Linear: #只将net模型中的线性层进行初始化
        nn.init.normal_(m.weight, std=0.01) #均值0和标准差0.01随机初始化权重

net.apply(init_weights); #初始化线性层的参数
~~~

#### flatten展平层

将张量进行展平操作，一张28x28像素灰度图片，可以理解是一个10x28的张量，使用展平层可以把这个2维张量拉成1维长度280(28*10=280)的张量进行输入

概括的说把图片拉成一条线那种输入的意思🐻

#### torch.init.normal_()

将tensor初始化，一般用于网络中参数weight初始化，初始化参数值符合正态分布。

函数中的参数：

- **tensor** -- 一般是权重weight矩阵
- **mean** -- 正态分布，默认均值为0
- **std** -- 方差

#### torch.nn.Module.apply(fn)

将一个函数fn递归地应用到模块自身以及该模块的每一个子模块中，该方法通常用来初始化一个模型中的参数

这里net只有一个线性层，说明只初始化线性层



 
