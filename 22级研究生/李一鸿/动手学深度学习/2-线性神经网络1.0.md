# 线性神经网络

## 线性回归

### 回归： 

什么是回归？回归、回归，让数据回归到或者恢复出某种规律。

而这种规律可以理解为输入和输出之间的关系。所以我觉得称为“回规”更合适🐶

这个规律可以用数学模型来表示，所以说回归是能为一个或多个自变量与因变量之间关系建模的一类方法

这个建模的数学模型，在统计学中通常被称为预测函数。y = f(x)

### 线性模型：

假设预测函数y = f(x) 形式为**y = wx + b**

其中w、b为参数，输入（x）和输出（y）之间存在一次函数关系（线性关系）， 这个回归模型为线性模型。

以书中买房子为例，现在我手头上有一堆一堆数据，房屋面积、房龄、房价😸

希望根据这些数据找到房屋面积、房龄与房屋价格的关系，当再来一个新房子时根据房屋面积和房龄利用刚刚的关系来预测房屋的价格

这个关系利用回归的方法来找，可以先做线性假设，建立线性模型。当然这只是一种假设，面积、房龄与价格可能具有线性关系，但关系不一定是线性的，模型还有很多很多种，但线性模型是最简单也是最流行的。这个模型对于咱现在这个问题也是绰绰有余的。

​								   						 	==**price=𝑤~area~⋅area + 𝑤~age~⋅age + 𝑏.**==

建立上式线性模型，其中价格为预测值，房龄和面积为特征值，𝑤~area~与 𝑤~age~分别是两个特征值的权重，*b*是偏置，偏置是指当所有特征都取值为0时，预测值应该为多少。 即使现实中不会有任何房子的面积是0或房龄正好是0年的情况，但仍然需要偏置项。 因为没有偏置项，模型的表达能力将受到限制。

ok😺给定一个数据集，它目标就是寻找模型的权重𝐰和偏置𝑏， 使得根据模型做出的预测大体符合数据里的真实价格。 那么如何找到这个合适的权重和偏置呢？

在找这个之前，还需要两个东西

（1）一种模型质量的度量方式； （2）一种能够更新模型以提高模型预测质量的方法

### 损失函数：

损失函数就是模型质量度量的一种方式

误差loss = 预测值 - 实际值

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220527185315448.png" alt="image-20220527185315448" style="zoom:67%;" />

即如上图所示，loss = 𝑦̂^(𝑖)^−𝑦^(𝑖)^，loss越小，预测越准确

但预测值与实际值有时大，有时又小（图中所选预测值大于实际值）

所以这个误差有时正有时负，只有它的绝对值才具有参考意义，故取平方误差来作为拟合程度的度量

可以得到**损失函数：**

𝑙^(𝑖)^(𝐰,𝑏)=1/2*(𝑦̂ ^(𝑖)^−𝑦^(𝑖)^)^2^.

常数1/2不会带来本质的差别，只是在形式上稍微简单一些 （因为当我们对损失函数求导后常数系数为1）。

而这只是在一个点上的损失函数，为了度量模型在整个数据集上的质量，我们需计算在训练集𝑛个样本上的损失均值（也等价于求和）即下列整个数据集的损失函数概括为：

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220527190441512.png" alt="image-20220527190441512" style="zoom: 50%;" />

==以上都是书中的例子，但是变量权重是向量啥的不是那么好理解，故再拿最简单的没有偏置的线性模型再解释一下==

设权重就是一个简单的变量未知数而不是一个向量，即 y = wx  这种普普通通线性模型为例🐻

它在训练集𝑛个样本上的损失均值函数有
$$
e = \frac{1}{n}\sum_{i=1}^{n}(wx_i - y_i)^2
$$
对这种线性模型的损失函数展开
$$
e = \frac{1}{n}\sum_{i=1}^{n}wx_i^2 - 2 \frac{1}{n}\sum_{i=1}^{n}x_iy_iw + \frac{1}{n}\sum_{i=1}^{n}y_i^2
$$
由于数据点的x~i~ 和y~i~ 以及总数𝑛是已知的，合并同类项可以简化为
$$
e =  aw^2 + bw + c
$$
其中a、b、c为常数

这个关于误差的简略式子就是机器学习中的代价函数，也称损失函数

自变量不再是x而是w，因变量也变成了误差e

那么这个代价函数怎么用呢？

还能怎么用！？这不就是一个普普通通的二次函数吗，在坐标系中它是一个抛物线，开口向上有最小值。

e取最小值即误差最小，就能说明此时w最优！

==但实际情况往往比这复杂太多，当预测函数如果有多个权重变量呢？对应代价函数的几何意义往往不是那么简单的抛物线，可能是三维空间的抛物面，甚至有可能是超曲面，这时就不太去能找到最理想的拟合==😿

但我们的目标，就是要找到最小的损失！！！

有了代价函数，怎么找到差不多能符合要求的最小值呢？那就要说一说**梯度下降法**了

### 梯度下降法：

梯度下降法就是能够更新模型以提高模型预测质量的方法

#### 梯度：

梯度的本意是一个向量，函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大（为该梯度的模），这里的梯度是损失函数的导数

#### 下降：

我们已经有损失函数了，为了保证这个损失最小就要找最小值，但实际问题训练样本分步千奇百怪，对应的代价函数千变万化。最小值是很难求得解析解的，所以需要合适的方法去寻找这个最小值，而这个过程就是梯度下降。

先假定一个权重初值，这个值是任意的一个数值，直觉告诉我们，只要选择向函数变化最快的方向走就能找到最值，由于我们是要最小值，所以沿函数变化最快的方向向下走就能找到最低点，即函数变化最快的方向为梯度的方向，而向下走就是下降。🙉

所以对于自变量沿着梯度下降，因变量的取值会越来越小，对于损失函数，即损失就越来越小

还可以形象的把损失函数理解为一座山，一个人在山上任意一个位置想走到山底，怎么走最快？

即每次沿最陡的方向向下走，就能很容易到达山底，而这个最陡的方向就是梯度。

#### 步长：

确定了方向以后就要前进迈步了，但迈多大的步呢？这个步长怎么得到呢？

先辈的智慧，步长 = 学习率*梯度

学习率是步长的超参数，超参数指不在训练过程更新的参数，超参数通常根据迭代的结果进行计算。

学习率确定也是有讲究的

学习率太小，每一次走的步长就很有限，就要走很多很多的步，因为计算梯度是一件很贵的事情，需要避免计算梯度，所以学习率不能取太小；当然也不能走太大，如果走的太大一步把最小值给迈过去了，使得求得的值一直在震荡，没办法收敛到一个最优值，是得不到最优解的。

所以学习率不能太大也不能太小，以后会有很多的教程来教怎么选取学习率，以后再说吧🐔

故权重的更迭可以概括为 新w = 旧w - 步长

然后不达目标不罢休，循环迭代，直到新w能保证损失函数损失符合要求。

#### 随机梯度下降：

梯度下降最简单的用法是计算损失函数关于模型参数的导数。

但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。

而遍历整个数据集来求导这个代价太大了！😿

因此，通常会在每次需要计算更新的时候随机抽取一小批样本， 这种变体叫做**小批量随机梯度下降**

简称MBGD，每下降一步，选用一小批样本参与计算，下降过程虽然比不上数据集全部计算下降的平稳有规律，但是快得多；虽然又比不上只随机抽取一个样本计算速度来的快，但更为精准。

所以相对来说，这种方法又快又稳！这种方法最科学。

#### 小结：

1. 定义损失函数
2. 选择起始点
3. 计算梯度
4. 按学习率前进
5. 重复3、4直至找到损失符合要求的点

所以从上面可以看出损失函数，起始点，梯度，学习率都是算法可以改进的方向