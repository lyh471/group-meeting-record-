# 深度学习计算

## 层和块

其实在构建模型的时候前面已经使用了很多层和块，但是也就是使用，如何利用pytorch灵活的自定义层和块还是一窍不通的，这一章就是用来学习如何灵活自主的构建神经网络

首先层我们知道，但块是什么？

对于多层感知机而言，整个模型及其组成是由多个层构成的。所以事实证明，研究讨论“比单个层大”但“比整个模型小”的组件是很有价值的。，而这可以概括为层组（多个层相连）。

例如，在计算机视觉中广泛流行的ResNet-152架构就有数百层， 这些层是由*层组*（groups of layers）的重复模式组成。 在其他的领域，如自然语言处理和语音， 层组以各种重复模式排列的类似架构现在也是普遍存在。

所以为了实现这些复杂的网络层组，我们引入了神经网络**块**的概念。

 **块**（block）可以描述单个层、由多个层组成的组件或整个模型本身。

使用块进行抽象的一个好处是可以将一些块组合成更大的组件， 这一过程通常是递归的， 通过定义代码来按需生成任意复杂度的块， 这样可以通过简洁的代码实现复杂的神经网络。

而从编程角度来说，“块”由类表示，并且该类的任何子类都必须定义一个将其输入转换为输出的前向传播函数， 并且必须存储任何必需的参数。

### 自定义块：

现在以4.3节的单隐藏层多层感知机为基准，将这个模型所包含的层封装到一个类成，构成一个块

这个模型利用顺序类容器由有以下定义

~~~py
import torch
from torch import nn #导入神经网络模组
from torch.nn import functional as F #导入模组中的functional模组并记为F
#该模组有大量计算函数,如ReLU激活函数等

net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
#单隐藏层多层感知机的模型
~~~

先将这个模型做成一个块，这个块包含这个多层感知机，即具有256个隐藏单元的隐藏层和一个10维输出层。

~~~py
class MLP(nn.Module): #块由类表示,并且它是神经网络模块的一个子类,可以概括为块是一个子神经网络
#该块的名称为MLP    
    
    #用模型参数声明层,这里,我们声明两个全连接的层
    def __init__(self):
        #调用MLP的父类Module的构造函数来执行必要的初始化
        #这样,在类实例化时也可以指定其他函数参数,	例如模型参数params（稍后将介绍）
        super().__init__()
        
        #该类有两个成员变量,隐藏层和输出层
        self.hidden = nn.Linear(20, 256)  #隐藏层
        self.out = nn.Linear(256, 10)  #输出层

    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出
    def forward(self, X):
        #注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。
        return self.out(F.relu(self.hidden(X)))
~~~

⚠️注意一些关键细节： 首先，我们自定义的构造函数__init__函数通过super().\_\_init\__() 调用父类的\_\_init__函数， 省去了重复编写模版代码的痛苦。 然后，我们实例化两个全连接层， 分别赋值给了self.hidden和self.out两个成员变量。==虽然定义了两个成员变量，但我们并没有想自定义那样按顺序在容器内排序，那么如何确定两个层的连接问题？没错，我们在下面的forward前向传播函数定义了如何计算，解决了层与层如何连接==

⚠️其次，除非我们实现一个新的运算符， 否则我们不必担心反向传播函数或参数初始化， 系统将自动生成这些。

~~~py
net = MLP() #调用构造函数实例化这个块
net(X) #net即为256个隐藏单元的隐藏层和一个10维输出层的多层感知机模型
~~~

块的一个主要优点是它的多功能性。 我们甚至还可以子类化块以创建层（如全连接层的类）、 整个模型（如上面的MLP类）或具有中等复杂度的各种组件。 这在卷积神经网络中很有用！

### 顺序块：

 在线性神经网络3.0中提到，Sequential类可以将多个层串联在一起。

我们可以自己定义一个简化版的顺序块MySequential，来起到类似的作用

对于该块只需要定义两个关键函数：

- 一种将块逐个追加到列表中的函数
- 一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。

下面自定义的MySequential与nn模组中的Sequential类具有相同的功能

~~~py
class MySequential(nn.Module):
    
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            #这里,module是Module子类的一个实例.我们把它保存在'Module'类的成员
            #变量_modules中.module的类型是OrderedDict
            self._modules[str(idx)] = module
            #按序插入,并将它自己的名字作为key

    def forward(self, X):
        #OrderedDict保证了按照成员添加的顺序遍历它们
        for block in self._modules.values():
            X = block(X) #按层序一层一层返回给X
        return X
~~~

当MySequential的前向传播函数被调用时， 每个添加的块都按照它们被添加的顺序执行。

现在可以使用我们的MySequential类重新实现多层感知机。

~~~py
net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
net(X)
~~~

### 前向传播函数执行代码：

Sequential类使模型构造变得简单，允许我们组合新的架构，而不必定义大量的类，只需要层与层按顺序叠放即可

 然而，并不是所有的架构都是简单的顺序架构。 当需要更强的灵活性时，我们还是需要定义自己的块。 

到目前为止， 我们网络中的所有操作都对网络的激活值及网络的参数起作用。

 然而，有时我们可能希望合并而不是使用上一层的结果和可更新参数的项，我们将这种参数称为**常数参数**

例如，我们需要一个计算函数 𝑓(𝐱,𝐰)=𝑐⋅𝐰⊤𝐱的层， 其中𝐱是输入， 𝐰是参数， 𝑐是某个在优化过程中没有更新的指定常量。 

因此我们实现了一个FixedHiddenMLP类，如下所示：

~~~py
class FixedHiddenMLP(nn.Module): #修改的一个隐藏层
    
    def __init__(self):
        super().__init__()
        #修改的隐藏层不计算梯度
        self.rand_weight = torch.rand((20, 20), requires_grad=False)
        self.linear = nn.Linear(20, 20) #层的形状

    def forward(self, X): #修正该隐藏层的前向传播
        #先将X传入,然后根据随机化的常数权重计算通过这个全连接层
        X = self.linear(X)
        # 使用创建的常量参数以及relu和mm函数
        X = F.relu(torch.mm(X, self.rand_weight) + 1)
        #矩阵乘法
        #+1是假设的偏移
        #该层的计算使用随机的常数参数与计算
        #复用全连接层,这相当于两个全连接层共享参数
        X = self.linear(X)
        
        # 控制流
        while X.abs().sum() > 1: #如果L1范数绝对值大于1的话,就继续除2除2除2,转换为一个很小的数
            X /= 2
        return X.sum() #最后返回的是X矩阵的和,是一个标量
~~~

在这个FixedHiddenMLP模型中，实现了一个隐藏层， 其权重（self.rand_weight）在实例化时被随机初始化，之后为常量。 这个权重不是一个模型参数，因此它永远不会被反向传播更新。 

然后，神经网络将这个固定层的输出通过一个全连接层。

注意，在返回输出之前，模型做了一些不寻常的事情： 它运行了一个while循环，在𝐿~1~范数大于1的条件下将输出向量除以2，直到它满足条件为止。 

最后，模型返回了X中所有项的和。

⚠️ 注意，此操作可能不会常用于在任何实际任务中， 没有什么实际意义，只是体现我们可以对某个层修改作为一个组件，来达到自定义网络的灵活性

为了更体现灵活性，在举一个例子，可以混合搭配各种组合块的方法，在下面的例子中，我们以一些想到的方法嵌套块。

~~~py
class NestMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),
                                 nn.Linear(64, 32), nn.ReLU())
        self.linear = nn.Linear(32, 16)

    def forward(self, X):
        return self.linear(self.net(X)) #三层嵌套

chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP()) #五层嵌套模型
chimera(X) #由最后一层可以得出,返回的是一个标量
~~~

## 参数管理

ok，在上一节已经定义好了我们的“块”，那么块中的参数怎么去访问？🦝之前我就想吐槽了，依靠深度学习框架来完成训练工作，但参数那么重要，并没有详细的介绍参数的具体使用细节

接下来，参数管理就从以下几个方面入手

- 访问参数，用于调试、诊断和可视化
- 参数初始化
- 在不同模型组件间共享参数

还是以单隐藏层的那个多层感知机举例

~~~py
import torch
from torch import nn

net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))
X = torch.rand(size=(2, 4))
net(X)

tensor([[0.2822],
        [0.2496]], grad_fn=<AddmmBackward0>
~~~

### 参数访问：

首先我们把每层的权重都拿出来，由于我们的net模型是一个Sequential类的对象，而Sequential类的对象可以把它看作是一个列表，所以利用下标net[2]就是最后的一个输出层，那个8*1的那层，打印一下看看

~~~py
print(net[2].state_dict())
#state是一个状态,从自动机的角度来讲,权重就是状态的一部分,因为权重（状态）可以被改变

#打印后显示
OrderedDict([('weight', tensor([[ 0.3149,  0.0068,  0.0585,  0.3311, -0.3486,  0.3373, -0.3053,  0.2185]])), ('bias', tensor([0.1071]))])
#OrderedDict可以理解为有序字典
#weight是一个8*1的矩阵
#bias是一个标量
~~~

最后一层的参数可以通过state_dict()拿出来

#### 目标参数：

当然也可以访问某一个具体的参数

~~~py
print(type(net[2].bias))
print(net[2].bias) #.bias是Parameter containing所包含的,不光只有偏移的数值,还有其对应的梯度
print(net[2].bias.data) #.bias.data才是真正的偏移值,并不包含梯度,常用于参数的初始化

<class 'torch.nn.parameter.Parameter'> #显示偏移的类型
Parameter containing: 
tensor([0.1071], requires_grad=True)
tensor([0.1071])
~~~

参数是复合的对象，包含值、梯度和额外信息。 这就是我们需要显式参数值的原因，所以我们可以利用weight.data来访问weight的数值，也可以利用weight.grad来访问梯度

~~~py
net[2].weight.grad == None #由于还没有进行反向传播,所以梯度仍为初始状态

True
~~~

#### 一次项访问所有参数：

我们也可以把网络中所有的Parameter全部拿出来

~~~py
print(*[(name, param.shape) for name, param in net[0].named_parameters()])
#*为解包,将后面的元组等分解成一个一个单元
#named_parameters()返回网络层的名字(字符串)和参数迭代器
print(*[(name, param.shape) for name, param in net.named_parameters()])
~~~

有了名字，当然也可以指定名字来访问参数

~~~py
net.state_dict()['2.bias'].data
#根据字符串的名字的键值来访问data
#最后一层的偏移
~~~

#### 从嵌套快收集参数：

当网络相互嵌套时，不在是按顺序来存放的网络怎么获得参数

首先定义一个嵌套网络：

~~~py
def block1():
    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                         nn.Linear(8, 4), nn.ReLU()) #很普通的两层网络

def block2(): #block2嵌套四个block1
    net = nn.Sequential()
    for i in range(4):
        # 在这里嵌套
        net.add_module(f'block {i}', block1()) #这样向容器中增加模组对每一块进行了命名
        									   #模块也有了名字而不是0,1,2,3.这是block 0...
    return net

rgnet = nn.Sequential(block2(), nn.Linear(4, 1)) #最后在连接一个线性层

#设计了网络,可以print出来

print(rgnet)

Sequential(
  (0): Sequential(
    (block 0): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 1): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 2): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 3): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
  )
  (1): Linear(in_features=4, out_features=1, bias=True)
)
~~~

这个网络的架构是不是清晰明了，当然可以从中可以看出有两层嵌套的结构

因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。

访问第一个主要的块中、第二个子块的第一层的偏置项

~~~py
rgnet[0][1][0].bias.data
~~~

所以组合成块的好处还有复杂的网络组成模块化，在打印上、访问上也很方便

### 参数初始化：

访问了权重和偏移，那么如何修改默认的初始化函数来获得新的初始化

之前用过了这些东西啦，现在在复习以下

~~~py
def init_normal(m): #正态分布
    if type(m) == nn.Linear: #如果模组是线性类,全连接层,才对权重进行初始化
        nn.init.normal_(m.weight, mean=0, std=0.01) #对weight做均值为0,方差为0.01的初始化
        nn.init.zeros_(m.bias) #bias权重置0
        #init.normal_()后跟下划线是init.normal()函数的用法,表示不是返回一个值而是原地替换掉
        
#这些所有函数都在init模组里面,init模组里面包含了大量初始化的函数        

net.apply(init_normal) #对所有net中的layer层,一个一个循环递归调用这个初始化函数
#如果是嵌套的话,就嵌套遍历
~~~

我们还可以将所有参数初始化为给定的常数，比如初始化为1。

~~~py
def init_constant(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 1) #和正态分布初始化一样,唯一的区别是将权重初始化唯一的值常数1
        nn.init.zeros_(m.bias)
        
net.apply(init_constant)
~~~

学习的时候可以这样设置，但设计网络的时候千万不要这样做！💀weight全为1不就相当于这一层无论有多少个神经元不都要等效为一个神经元吗

我们还可以对某些块应用不同的初始化方法

例如，下面我们使用Xavier初始化方法初始化第一个神经网络层， 然后将第三个神经网络层初始化为常量值42。

利用apply对不同的层做不同的事情

~~~py
def xavier(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight) #xavier均匀分布初始化,哇init这个方法真的好
        #还有一个init.xavier_normal()
#xavier初始化就是避免数值不稳定的一个初始化方法
        
def init_42(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 42)

net[0].apply(xavier)
net[2].apply(init_42)
~~~

#### 自定义初始化：

还可以做更加奇怪的操作，深度学习框架没有提供我们需要的初始化方法，我们可以自定义初始化

以书上的例子为例

~~~py
def my_init(m):
    if type(m) == nn.Linear:
        print("Init", *[(name, param.shape) #首先初始化时print以下,说明我们要初始化啦！
                        for name, param in m.named_parameters()][0])
        #名字叫啥,形状是什么
        nn.init.uniform_(m.weight, -10, 10) #均匀分布初始化
        m.weight.data *= m.weight.data.abs() >= 5 #保留绝对值大于5的权重,如果绝对值超过5就设置为0

net.apply(my_init)
net[0].weight[:2]
~~~

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220622212833054.png" alt="image-20220622212833054" style="zoom: 50%;" />

还有更简单更暴力的方法，直接通过参数.data做替换

~~~py
net[0].weight.data[:] += 1 #权重所有值+1
net[0].weight.data[0, 0] = 42 #权重第一个元素赋值为42
~~~

### 参数绑定

参数管理的一个小应用，我们想在一些层之间共享参数

希望两个层共享一个权重，共享权重说明权重参数是一样的

如何保证参数一样呢？这就引入一个新的概念**参数绑定**

~~~py
# 我们需要给共享层一个名称，以便可以引用它的参数
shared = nn.Linear(8, 8) #先将共享参数的层构造出来

net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                    shared, nn.ReLU(),
                    shared, nn.ReLU(),
                    nn.Linear(8, 1))
#这样我们在构建网络时,第二个隐藏层和第三个隐藏层是共享权重的,只有第一个和最后一个是自己的
#理论上来说,不管怎么更新这个神经网络,他的第二个和第三个层的权重是一致的
#同一个实例,指向同一个内存,这两个层在一起还有一个好听的名字叫做稠密层.

net(X)
# 检查参数是否相同
print(net[2].weight.data[0] == net[4].weight.data[0])

"""tensor([True, True, True, True, True, True, True, True])"""

net[2].weight.data[0, 0] = 100
# 确保它们实际上是同一个对象，而不只是有相同的值
print(net[2].weight.data[0] == net[4].weight.data[0])

"""tensor([True, True, True, True, True, True, True, True])"""
~~~

这就是如何在不同的网络之间共享权重的方法

## 自定义层

前面讲了如何自定义块，块可以理解为一个子神经网络，而子神经网络有很多层构成。

==深度学习成功背后的一个因素是神经网络的灵活性： 我们可以用创造性的方式组合不同的层，从而设计出适用于各种任务的架构。==

下面介绍如何自定义层

### 不带参数的层：

自定义层和自定义块没有什么本质区别，因为自定义层也是nn模组的一个子类

~~~py
import torch
import torch.nn.functional as F
from torch import nn


class CenteredLayer(nn.Module): #无参数的层
    def __init__(self): #初始化函数也不用做什么事情
        super().__init__() #不用调用这个函数也没有关系,它会默认加上(python3才可以)

    def forward(self, X): #和自定义块是一样的,同样需要定义forward前向传播函数
        return X - X.mean() #这里教学使用,将均值减掉,均值为0
    
#使用
layer = CenteredLayer()
layer(torch.FloatTensor([1, 2, 3, 4, 5]))

tensor([-2., -1.,  0.,  1.,  2.])

#同时可以将层作为组件合并到复杂的模型中
net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())
Y = net(torch.rand(4, 8))
Y.mean()

tensor(9.3132e-10, grad_fn=<MeanBackward0>)
#这里看作均值为0.由于我们处理的是浮点数,因为存储精度的原因,仍然可能会看到一个非常小的非零数
~~~

### 带参数的层：

如果层想要带有参数那如何定义呢？无论是深度框架的层，还是自定义的层，都应该是**Parameter**这个类的实例，所以参数设置就要调用相应的Parameter类的构造函数来实例化

~~~py
class MyLinear(nn.Module): #自定义带参数的层
    def __init__(self, in_units, units): #in_units输入的维度, units输出的维度
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, units))
        self.bias = nn.Parameter(torch.randn(units,))
        #利用nn.Parameter()构造函数实例化生成对应的参数
        #该类生成的对象不光有数值,还有梯度等其他额外信息
        #使用最简单的-1~+1作为初始化
    def forward(self, X): #定义该层的前向传播,前向传播的计算说明了该层如何连接(计算)各个单元
        linear = torch.matmul(X, self.weight.data) + self.bias.data
        #普通线性层,全连接计算
        return F.relu(linear) #将结果relu激活
    
    
linear = MyLinear(5, 3)
linear.weight

"""
Parameter containing:
tensor([[ 1.9054, -3.4102, -0.9792],
        [ 1.5522,  0.8707,  0.6481],
        [ 1.0974,  0.2568,  0.4034],
        [ 0.1416, -1.1389,  0.5875],
        [-0.7209,  0.4432,  0.1222]], requires_grad=True)
"""

linear(torch.rand(2, 5))

tensor([[2.4784, 0.0000, 0.8991],
        [3.6132, 0.0000, 1.1160]])
~~~

还可以使用自定义层参与复杂网络的构造

~~~py
net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))
net(torch.rand(2, 64))

tensor([[0.],
        [0.]])
~~~

## 读写文件

我们已经学习如何处理数据，如何构建、训练和测试深度学习模型。但是训练好的东西我们如何来存储呢？所以接下来需要学习如何读写文件，学习如何加载和存储权重和整个模型。

### 加载和保存张量：

对于单个张量，我们可以直接调用**load**和**save**函数分别读写它们

 这两个函数都要求我们提供一个名称，**save**要求将要保存的变量作为输入

~~~py
import torch
from torch import nn
from torch.nn import functional as F

x = torch.arange(4)
torch.save(x, 'x-file') #将张量保存在文件x-file中,并且与当前代码文件下的当前目录下

#通过load可以将文件的数据读回内存
x2 = torch.load('x-file')
x2
tensor([0, 1, 2, 3])
~~~

是不是好奇这个==x-file==的类型是什么？.txt？.py？.......都不是，这个x-file的类型就是文件！

所以我也不知道这是什么🐂🐎...等有机会在深入了解一下吧

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220624230916041.png" alt="image-20220624230916041" style="zoom: 50%;" />

离谱...所以文件的文件类型是文件

同样的，可以不光简单存储一个张量，还可以存储一个张量列表

~~~py
y = torch.zeros(4)
torch.save([x, y],'x-files')
x2, y2 = torch.load('x-files')
(x2, y2)

(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))
~~~

甚至可以写入或读取从字符串映射到张量的字典。 当我们要读取或写入模型中的所有权重时，这很方便。

~~~py
mydict = {'x': x, 'y': y}
torch.save(mydict, 'mydict')
mydict2 = torch.load('mydict')
mydict2

{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}
~~~

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220624231249838.png" alt="image-20220624231249838" style="zoom:50%;" />

通过**torch.save/load**对单个张量/列表/字典进行存储很方便，这就是最简单的加载和存储

### 加载和保护模型参数：

接下来，假设我们有个MLP的话该怎么存储？就是有一个多层感知机模型如何存储？

首先我们要明确对于一个神经网络，我们需要存储什么东西，**pytorch**与其他深度学习框架不同，它不能把模型的定义存下来，当然它也有其他办法可以存如*torchscript*这里就不在介绍

所以对于一个MLP的存储，我们只需将它的权重存下来就可以了（权重可以近似认为是神经元），而如何连接计算（前向传播）那部分就不存储了，因为前向传播涉及模型的定义，我们只需建立一个相同的模型然后将存储的参数覆盖掉新建的模型的参数即可获得我们保存的模型。

ok，首先定义一个MLP，它有两个线性层，其他情况和之前类似

~~~py
class MLP(nn.Module): #感知机模型
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256) #两个全连接层
        self.output = nn.Linear(256, 10)

    def forward(self, x):
        return self.output(F.relu(self.hidden(x))) #普通的双层线性计算

net = MLP()
X = torch.randn(size=(2, 20)) #用于测试,给定一个随机的X,计算出来Y,用于验证
Y = net(X)
~~~

在参数管理那节中，我们可以通过**state_dict()**来得到所有权重字符串以及到它对应的映射（state_dict()返回的是一个有序字典）

那么我们就将这个带有权重的有序字典及模型的参数存储在一个叫做“mlp.params”的文件中

~~~py
torch.save(net.state_dict(), 'mlp.params')
~~~

这也就是说整个MLP中所有参数存成一个字典，这个字典是参数名字以及数值对应的映射

⚠️需要注意的是，在save过程中主动了加上了文件的后缀**.params**，所以文件类型是params....又是一个没见过的文件后缀，那如果不声明是不是存储的文件类型就是文件呢？有待验证

<img src="C:\Users\lyh471\AppData\Roaming\Typora\typora-user-images\image-20220624235701818.png" alt="image-20220624235701818" style="zoom:50%;" />

ok，模型的参数保存在了文件中，那么如何**load**回来呢？

load时我们需要注意，我们不光需要加载参数，我们还需要定义一个相似的模型，这也就是pytorch不好的地方，无法将模型定义存储下来😿

~~~py
clone = MLP() #重定义初始化模型,当然新模型的参数是随机好的
clone.load_state_dict(torch.load('mlp.params')) 
#调用load_state_dict(),从文件中读出这个字典,覆盖掉我们初始化的随机参数

clone.eval() #对模型进行显示评估

#模型形状
MLP(
  (hidden): Linear(in_features=20, out_features=256, bias=True)
  (output): Linear(in_features=256, out_features=10, bias=True)
)
~~~

那么如何验证我们这个模型是否存储正确呢？

利用刚刚输入X得到的输出的Y进行判断即可，如果两个输出是相等的，说明两个模型相等

~~~py
Y_clone = clone(X)
Y_clone == Y

"""
tensor([[True, True, True, True, True, True, True, True, True, True],
        [True, True, True, True, True, True, True, True, True, True]])
"""
#这说明这两个模型的参数是一样的,这个过程就到达了模型的存储和加载
~~~



**下一节是GPU....我没有GPU，先标记一下之后在学吧！！！，那么第五章就结束咯😸**
